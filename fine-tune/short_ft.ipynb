{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba59a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Fine-tuning: Training Mistral 7B to produce short responses\n",
    "# Comparison experiment for neologism vs fine-tuning study\n",
    "#\n",
    "# This script uses the SAME hyperparameters and loss function (DPO + APO-up)\n",
    "# as the neologism training for a fair comparison.\n",
    "#\n",
    "# Usage: Run cells sequentially in Google Colab with GPU runtime\n",
    "\n",
    "# %% [markdown]\n",
    "# # LoRA Fine-tuning: Short Response Behavior\n",
    "# \n",
    "# Training Mistral-7B to always produce concise responses using LoRA.\n",
    "# Uses identical hyperparameters and DPO+APO-up loss as neologism training.\n",
    "\n",
    "# %% \n",
    "# ### Dependencies\n",
    "# !pip install -q transformers accelerate bitsandbytes torch peft\n",
    "\n",
    "# %%\n",
    "# ### Configuration - EASY TO MODIFY\n",
    "LORA_RANK = 8          # Change to 1 for rank-1 experiment\n",
    "LORA_ALPHA = 16        # Typically 2x rank (change to 2 for rank-1)\n",
    "LORA_DROPOUT = 0.0     # Keep at 0 for small dataset\n",
    "TARGET_MODULES = [\"q_proj\", \"v_proj\"]  # Standard for attention tuning\n",
    "\n",
    "# Hyperparameters - MATCHED TO NEOLOGISM TRAINING\n",
    "LR = 1e-4\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 1\n",
    "ACCUMULATION_STEPS = 10  # Effective batch size = 10\n",
    "MAX_LENGTH = 1024\n",
    "BETA = 0.2  # DPO beta\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "DATA_FILE = \"short_ft.jsonl\"  # Training data without ~short in prompts\n",
    "\n",
    "# %%\n",
    "# ### Imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from google.colab import files\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %%\n",
    "# ### Load training data\n",
    "uploaded = files.upload()  # Upload short_ft.jsonl\n",
    "\n",
    "examples = []\n",
    "with open(DATA_FILE, \"r\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            examples.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(examples)} examples.\")\n",
    "print(f\"First example prompt: {examples[0]['prompt'][:100]}...\")\n",
    "\n",
    "# %%\n",
    "# ### Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded. Vocab size: {len(tokenizer)}\")\n",
    "\n",
    "# %%\n",
    "# ### Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=LORA_RANK,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# For comparison with neologism:\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTrainable parameters: {trainable_params:,}\")\n",
    "print(f\"Neologism had: 4,096 parameters\")\n",
    "print(f\"Ratio: {trainable_params / 4096:.1f}x more parameters than neologism\")\n",
    "\n",
    "# %%\n",
    "# ### Store reference model state for DPO\n",
    "# We store the initial LoRA weights to compute reference log probabilities\n",
    "# This is analogous to storing ref_embedding in the neologism training\n",
    "\n",
    "ref_lora_state = {}\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:  # Only LoRA parameters\n",
    "        ref_lora_state[name] = param.data.clone().detach()\n",
    "\n",
    "print(f\"Stored {len(ref_lora_state)} reference LoRA parameter tensors\")\n",
    "\n",
    "# %%\n",
    "# ### Loss functions (IDENTICAL to neologism training)\n",
    "\n",
    "def get_sequence_logprob(model, input_ids, attention_mask, response_start_idx):\n",
    "    \"\"\"\n",
    "    Compute log probability of the response portion of a sequence.\n",
    "    response_start_idx: index where the response tokens begin\n",
    "    \"\"\"\n",
    "    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Shift for next-token prediction\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = input_ids[:, 1:].contiguous()\n",
    "\n",
    "    # Compute log probs\n",
    "    log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "    token_log_probs = torch.gather(log_probs, dim=-1, index=shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    # Mask: only count response tokens\n",
    "    response_mask = torch.zeros_like(token_log_probs)\n",
    "    response_mask[:, response_start_idx-1:] = attention_mask[:, response_start_idx:]\n",
    "\n",
    "    # Sum log probs over response\n",
    "    seq_log_prob = (token_log_probs * response_mask).sum(dim=-1)\n",
    "    return seq_log_prob\n",
    "\n",
    "\n",
    "def compute_ref_logprob(model, ref_state, input_ids, attention_mask, response_start_idx):\n",
    "    \"\"\"\n",
    "    Compute logprob using reference LoRA weights (swap in ref, compute, swap back).\n",
    "    Analogous to compute_ref_logprob in neologism training.\n",
    "    \"\"\"\n",
    "    # Store current LoRA weights\n",
    "    current_state = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            current_state[name] = param.data.clone()\n",
    "    \n",
    "    # Swap in reference weights\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in ref_state:\n",
    "                param.data.copy_(ref_state[name])\n",
    "    \n",
    "    # Compute reference logprob\n",
    "    with torch.no_grad():\n",
    "        ref_logprob = get_sequence_logprob(model, input_ids, attention_mask, response_start_idx)\n",
    "    \n",
    "    # Swap back current weights\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in current_state:\n",
    "                param.data.copy_(current_state[name])\n",
    "    \n",
    "    return ref_logprob\n",
    "\n",
    "\n",
    "def dpo_apo_loss(logp_chosen, logp_rejected, ref_logp_chosen, ref_logp_rejected, beta=BETA):\n",
    "    \"\"\"\n",
    "    DPO + APO-up loss (IDENTICAL to neologism training):\n",
    "    t1 = -log(sigmoid(beta * (logp_c - logp_r - (ref_logp_c - ref_logp_r))))\n",
    "    t2 = -log(sigmoid(beta * (logp_c - ref_logp_c)))\n",
    "    \"\"\"\n",
    "    # DPO term\n",
    "    logit_diff = logp_chosen - logp_rejected - (ref_logp_chosen - ref_logp_rejected)\n",
    "    t1 = -F.logsigmoid(beta * logit_diff)\n",
    "\n",
    "    # APO-up term\n",
    "    t2 = -F.logsigmoid(beta * (logp_chosen - ref_logp_chosen))\n",
    "\n",
    "    return t1 + t2\n",
    "\n",
    "print(\"Loss functions defined (identical to neologism training).\")\n",
    "\n",
    "# %%\n",
    "# ### Data preparation (IDENTICAL to neologism training)\n",
    "\n",
    "def prepare_example(tokenizer, prompt, response, max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Tokenize prompt + response, return input_ids, attention_mask, and response start index.\n",
    "    \"\"\"\n",
    "    prompt_ids = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "    response_ids = tokenizer.encode(response, add_special_tokens=False)\n",
    "\n",
    "    full_ids = prompt_ids + response_ids\n",
    "\n",
    "    if len(full_ids) > max_length:\n",
    "        full_ids = full_ids[:max_length]\n",
    "\n",
    "    response_start_idx = len(prompt_ids)\n",
    "\n",
    "    input_ids = torch.tensor([full_ids])\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "    return input_ids, attention_mask, response_start_idx\n",
    "\n",
    "print(\"Data utilities defined.\")\n",
    "\n",
    "# %%\n",
    "# ### Training loop (IDENTICAL structure to neologism training)\n",
    "\n",
    "optimizer = AdamW(\n",
    "    [p for p in model.parameters() if p.requires_grad], \n",
    "    lr=LR\n",
    ")\n",
    "\n",
    "model.train()\n",
    "losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_losses = []\n",
    "    random.shuffle(examples)\n",
    "\n",
    "    pbar = tqdm(range(0, len(examples), BATCH_SIZE), desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    for batch_idx in pbar:\n",
    "        batch_examples = examples[batch_idx:batch_idx + BATCH_SIZE]\n",
    "        batch_loss = 0.0\n",
    "\n",
    "        for ex in batch_examples:\n",
    "            prompt = ex[\"prompt\"]\n",
    "            chosen = ex[\"chosen\"]\n",
    "            rejected = ex[\"rejected\"]\n",
    "\n",
    "            # Prepare examples\n",
    "            c_ids, c_mask, c_start = prepare_example(tokenizer, prompt, chosen)\n",
    "            r_ids, r_mask, r_start = prepare_example(tokenizer, prompt, rejected)\n",
    "\n",
    "            c_ids, c_mask = c_ids.to(model.device), c_mask.to(model.device)\n",
    "            r_ids, r_mask = r_ids.to(model.device), r_mask.to(model.device)\n",
    "\n",
    "            # Forward pass\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logp_chosen = get_sequence_logprob(model, c_ids, c_mask, c_start)\n",
    "                logp_rejected = get_sequence_logprob(model, r_ids, r_mask, r_start)\n",
    "\n",
    "                # Reference model logprobs\n",
    "                ref_logp_chosen = compute_ref_logprob(model, ref_lora_state, c_ids, c_mask, c_start)\n",
    "                ref_logp_rejected = compute_ref_logprob(model, ref_lora_state, r_ids, r_mask, r_start)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = dpo_apo_loss(logp_chosen, logp_rejected, ref_logp_chosen, ref_logp_rejected)\n",
    "                loss = loss.mean() / BATCH_SIZE\n",
    "\n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            batch_loss += loss.item() * BATCH_SIZE\n",
    "\n",
    "            # Clear cache\n",
    "            del c_ids, c_mask, r_ids, r_mask, logp_chosen, logp_rejected\n",
    "            del ref_logp_chosen, ref_logp_rejected, loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Update weights after accumulation\n",
    "        if (batch_idx // BATCH_SIZE + 1) % ACCUMULATION_STEPS == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                [p for p in model.parameters() if p.requires_grad], \n",
    "                1.0\n",
    "            )\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        epoch_losses.append(batch_loss)\n",
    "        pbar.set_postfix({\"loss\": f\"{batch_loss:.4f}\"})\n",
    "\n",
    "    # Final update for remaining gradients\n",
    "    if len(epoch_losses) % ACCUMULATION_STEPS != 0:\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            [p for p in model.parameters() if p.requires_grad], \n",
    "            1.0\n",
    "        )\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1} avg loss: {avg_loss:.4f}\")\n",
    "\n",
    "# %%\n",
    "# ### Plot training loss\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, EPOCHS+1), losses, 'b-o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.title(f'LoRA (rank={LORA_RANK}) Training Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# ### Test the fine-tuned model\n",
    "model.eval()\n",
    "\n",
    "test_prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain the theory of relativity.\",\n",
    "    \"What causes rain?\",\n",
    "    \"How do computers work?\",\n",
    "    \"What is the meaning of life?\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"TESTING LORA FINE-TUNED MODEL (rank={LORA_RANK})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for p in test_prompts:\n",
    "    inputs = tokenizer(p, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=200, \n",
    "            do_sample=True, \n",
    "            temperature=0.7, \n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    response = tokenizer.decode(out[0], skip_special_tokens=True)[len(p):]\n",
    "    word_count = len(response.split())\n",
    "    print(f\"\\nQ: {p}\")\n",
    "    print(f\"A: {response}\")\n",
    "    print(f\"[Word count: {word_count}]\")\n",
    "\n",
    "# %%\n",
    "# ### Compare with base model (optional - load fresh model)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON: Loading base model for reference...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Disable LoRA temporarily to see base model behavior\n",
    "model.disable_adapter_layers()\n",
    "\n",
    "for p in test_prompts[:2]:  # Just test first 2 to save time\n",
    "    inputs = tokenizer(p, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=200, \n",
    "            do_sample=True, \n",
    "            temperature=0.7, \n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    response = tokenizer.decode(out[0], skip_special_tokens=True)[len(p):]\n",
    "    word_count = len(response.split())\n",
    "    print(f\"\\nQ: {p}\")\n",
    "    print(f\"A (BASE): {response}\")\n",
    "    print(f\"[Word count: {word_count}]\")\n",
    "\n",
    "# Re-enable LoRA\n",
    "model.enable_adapter_layers()\n",
    "print(\"\\nLoRA adapters re-enabled.\")\n",
    "\n",
    "# %%\n",
    "# ### Save LoRA adapter weights\n",
    "# Note: This saves ONLY the LoRA weights, not the full model\n",
    "# Much smaller than full model checkpoint\n",
    "\n",
    "save_path = f\"lora_short_rank{LORA_RANK}\"\n",
    "model.save_pretrained(save_path)\n",
    "print(f\"Saved LoRA adapter to {save_path}/\")\n",
    "\n",
    "# Also save config for reference\n",
    "config_info = {\n",
    "    \"lora_rank\": LORA_RANK,\n",
    "    \"lora_alpha\": LORA_ALPHA,\n",
    "    \"target_modules\": TARGET_MODULES,\n",
    "    \"base_model\": MODEL_NAME,\n",
    "    \"lr\": LR,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"beta\": BETA,\n",
    "    \"training_examples\": len(examples),\n",
    "}\n",
    "\n",
    "with open(f\"{save_path}/training_config.json\", \"w\") as f:\n",
    "    json.dump(config_info, f, indent=2)\n",
    "\n",
    "print(f\"Saved training config to {save_path}/training_config.json\")\n",
    "\n",
    "# Download the adapter\n",
    "import shutil\n",
    "shutil.make_archive(save_path, 'zip', save_path)\n",
    "files.download(f\"{save_path}.zip\")\n",
    "\n",
    "# %%\n",
    "# ### Verify LoRA weights changed\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"VERIFICATION: Checking that LoRA weights updated\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "total_diff = 0.0\n",
    "num_params = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if name in ref_lora_state:\n",
    "        diff = (param.data - ref_lora_state[name].to(param.device)).norm().item()\n",
    "        total_diff += diff\n",
    "        num_params += 1\n",
    "        if diff > 1e-6:\n",
    "            print(f\"  {name}: L2 diff = {diff:.6f} ✓\")\n",
    "        else:\n",
    "            print(f\"  {name}: unchanged ✗\")\n",
    "\n",
    "print(f\"\\nTotal L2 difference across all LoRA params: {total_diff:.6f}\")\n",
    "if total_diff > 1e-4:\n",
    "    print(\"✓ SUCCESS: LoRA parameters were updated during training\")\n",
    "else:\n",
    "    print(\"✗ WARNING: LoRA parameters may not have updated properly\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Loading saved LoRA weights later\n",
    "# \n",
    "# To load the saved LoRA adapter in a new session:\n",
    "# ```python\n",
    "# from peft import PeftModel\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# \n",
    "# # Load base model\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "#     load_in_8bit=True,\n",
    "# )\n",
    "# \n",
    "# # Load LoRA adapter\n",
    "# model = PeftModel.from_pretrained(base_model, \"lora_short_rank8\")\n",
    "# ```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
