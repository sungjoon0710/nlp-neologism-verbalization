{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4c56960e",
      "metadata": {
        "id": "4c56960e"
      },
      "source": [
        "## LoRA fine-tuning on Mistral with similar training data to the neologism learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "da670d6d",
      "metadata": {
        "id": "da670d6d",
        "outputId": "f53a0620-ff63-4e25-d3da-96477f83d0dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes torch peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e292b219",
      "metadata": {
        "id": "e292b219",
        "outputId": "84aa0069-324e-4c93-8d66-d46e0d214258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-81931241-d7ed-4574-baca-2b1883200b49\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-81931241-d7ed-4574-baca-2b1883200b49\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving short_ft.jsonl to short_ft.jsonl\n"
          ]
        }
      ],
      "source": [
        "# ### Configuration\n",
        "LORA_RANK = 8\n",
        "LORA_ALPHA = 2 * LORA_RANK\n",
        "LORA_DROPOUT = 0.0     # Keep at 0 for small dataset\n",
        "TARGET_MODULES = [\"q_proj\", \"v_proj\"]  # Standard for attention tuning\n",
        "\n",
        "# Hyperparameters - MATCHED TO NEOLOGISM TRAINING\n",
        "LR = 1e-4\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 1\n",
        "ACCUMULATION_STEPS = 10  # Effective batch size = 10\n",
        "MAX_LENGTH = 1024\n",
        "BETA = 0.2  # DPO beta\n",
        "\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "DATA_FILE = \"short_ft.jsonl\"  # Training data without ~short in prompts\n",
        "\n",
        "# %%\n",
        "# ### Imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from google.colab import files\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import random\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# %%\n",
        "# ### Load training data\n",
        "uploaded = files.upload()  # Upload short_ft.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ba59a98",
      "metadata": {
        "id": "9ba59a98",
        "outputId": "399c979b-fa48-41f0-b4db-0dd17791e3f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237,
          "referenced_widgets": [
            "d8fa81fb309f4da186b95f2208f5f72d",
            "712bb34b8dab410f8466d8be549ddcb6",
            "4ba422a1e52e42b8b9401d8c3ae9bbe4",
            "9056b2d8ee0b45f1b961940bc7470665",
            "ed97905c0fd44e1c8302384b747b9fd7",
            "10b46a0dfc2e4f888825cab89fe37e74",
            "1358fb65548c42a480a026134e3b955d",
            "d0b7f5643d2b41dfbb8cb72669824b2c",
            "9b0fabb8648e49cbae5a4fb45614033b",
            "dc83de7371f648d7a25eebe3c437eff8",
            "4df42dbb1173470cb5b1e455d910dfaf",
            "322d77ac1f9b4fd18bda8aac82bb9a6f",
            "62fc65648e08482d802b8d47600f41c9",
            "e0206ab6ad5647609583c617986c60cf",
            "73d097622ee545f5b6f0de20e42ee8d1",
            "70e149e381474fa3b835b0faeff37586",
            "1cd031a30a84442ba562b1e67eac2b00",
            "11567ae394ac42988cfcdc80b06c22aa",
            "a02a9d97ca5845d783271d93575d899d",
            "82e70b5c07ec4ac49388cc0976004bf5",
            "e6823909d8d1450283dda5452a120f7c",
            "533a972919424521891f260dbcdc4430"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1030 examples.\n",
            "First example prompt: Can brain cells move? By movement I mean long distance migration (preferably within the brain only)....\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8fa81fb309f4da186b95f2208f5f72d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "322d77ac1f9b4fd18bda8aac82bb9a6f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "examples = []\n",
        "with open(DATA_FILE, \"r\") as f:\n",
        "    for line in f:\n",
        "        if line.strip():\n",
        "            examples.append(json.loads(line))\n",
        "\n",
        "print(f\"Loaded {len(examples)} examples.\")\n",
        "print(f\"First example prompt: {examples[0]['prompt'][:100]}...\")\n",
        "\n",
        "# %%\n",
        "# ### Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True,\n",
        ")\n",
        "\n",
        "print(f\"Model loaded. Vocab size: {len(tokenizer)}\")\n",
        "\n",
        "# %%\n",
        "# ### Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=LORA_RANK,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    target_modules=TARGET_MODULES,\n",
        "    bias=\"none\",\n",
        ")\n",
        "\n",
        "# Apply LoRA to model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# For comparison with neologism:\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nTrainable parameters: {trainable_params:,}\")\n",
        "print(f\"Neologism had: 4,096 parameters\")\n",
        "print(f\"Ratio: {trainable_params / 4096:.1f}x more parameters than neologism\")\n",
        "\n",
        "# %%\n",
        "# ### Store reference model state for DPO\n",
        "# We store the initial LoRA weights to compute reference log probabilities\n",
        "# This is analogous to storing ref_embedding in the neologism training\n",
        "\n",
        "ref_lora_state = {}\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:  # Only LoRA parameters\n",
        "        ref_lora_state[name] = param.data.clone().detach()\n",
        "\n",
        "print(f\"Stored {len(ref_lora_state)} reference LoRA parameter tensors\")\n",
        "\n",
        "# %%\n",
        "# ### Loss functions (IDENTICAL to neologism training)\n",
        "\n",
        "def get_sequence_logprob(model, input_ids, attention_mask, response_start_idx):\n",
        "    \"\"\"\n",
        "    Compute log probability of the response portion of a sequence.\n",
        "    response_start_idx: index where the response tokens begin\n",
        "    \"\"\"\n",
        "    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Shift for next-token prediction\n",
        "    shift_logits = logits[:, :-1, :].contiguous()\n",
        "    shift_labels = input_ids[:, 1:].contiguous()\n",
        "\n",
        "    # Compute log probs\n",
        "    log_probs = F.log_softmax(shift_logits, dim=-1)\n",
        "    token_log_probs = torch.gather(log_probs, dim=-1, index=shift_labels.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    # Mask: only count response tokens\n",
        "    response_mask = torch.zeros_like(token_log_probs)\n",
        "    response_mask[:, response_start_idx-1:] = attention_mask[:, response_start_idx:]\n",
        "\n",
        "    # Sum log probs over response\n",
        "    seq_log_prob = (token_log_probs * response_mask).sum(dim=-1)\n",
        "    return seq_log_prob\n",
        "\n",
        "\n",
        "def compute_ref_logprob(model, ref_state, input_ids, attention_mask, response_start_idx):\n",
        "    \"\"\"\n",
        "    Compute logprob using reference LoRA weights (swap in ref, compute, swap back).\n",
        "    Analogous to compute_ref_logprob in neologism training.\n",
        "    \"\"\"\n",
        "    # Store current LoRA weights\n",
        "    current_state = {}\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            current_state[name] = param.data.clone()\n",
        "\n",
        "    # Swap in reference weights\n",
        "    with torch.no_grad():\n",
        "        for name, param in model.named_parameters():\n",
        "            if name in ref_state:\n",
        "                param.data.copy_(ref_state[name])\n",
        "\n",
        "    # Compute reference logprob\n",
        "    with torch.no_grad():\n",
        "        ref_logprob = get_sequence_logprob(model, input_ids, attention_mask, response_start_idx)\n",
        "\n",
        "    # Swap back current weights\n",
        "    with torch.no_grad():\n",
        "        for name, param in model.named_parameters():\n",
        "            if name in current_state:\n",
        "                param.data.copy_(current_state[name])\n",
        "\n",
        "    return ref_logprob\n",
        "\n",
        "\n",
        "def dpo_apo_loss(logp_chosen, logp_rejected, ref_logp_chosen, ref_logp_rejected, beta=BETA):\n",
        "    \"\"\"\n",
        "    DPO + APO-up loss (IDENTICAL to neologism training):\n",
        "    t1 = -log(sigmoid(beta * (logp_c - logp_r - (ref_logp_c - ref_logp_r))))\n",
        "    t2 = -log(sigmoid(beta * (logp_c - ref_logp_c)))\n",
        "    \"\"\"\n",
        "    # DPO term\n",
        "    logit_diff = logp_chosen - logp_rejected - (ref_logp_chosen - ref_logp_rejected)\n",
        "    t1 = -F.logsigmoid(beta * logit_diff)\n",
        "\n",
        "    # APO-up term\n",
        "    t2 = -F.logsigmoid(beta * (logp_chosen - ref_logp_chosen))\n",
        "\n",
        "    return t1 + t2\n",
        "\n",
        "print(\"Loss functions defined (identical to neologism training).\")\n",
        "\n",
        "# %%\n",
        "# ### Data preparation (IDENTICAL to neologism training)\n",
        "\n",
        "def prepare_example(tokenizer, prompt, response, max_length=MAX_LENGTH):\n",
        "    \"\"\"\n",
        "    Tokenize prompt + response, return input_ids, attention_mask, and response start index.\n",
        "    \"\"\"\n",
        "    prompt_ids = tokenizer.encode(prompt, add_special_tokens=True)\n",
        "    response_ids = tokenizer.encode(response, add_special_tokens=False)\n",
        "\n",
        "    full_ids = prompt_ids + response_ids\n",
        "\n",
        "    if len(full_ids) > max_length:\n",
        "        full_ids = full_ids[:max_length]\n",
        "\n",
        "    response_start_idx = len(prompt_ids)\n",
        "\n",
        "    input_ids = torch.tensor([full_ids])\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "\n",
        "    return input_ids, attention_mask, response_start_idx\n",
        "\n",
        "print(\"Data utilities defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b65dc7e9",
      "metadata": {
        "id": "b65dc7e9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# %%\n",
        "# ### Training loop (IDENTICAL structure to neologism training)\n",
        "\n",
        "optimizer = AdamW(\n",
        "    [p for p in model.parameters() if p.requires_grad],\n",
        "    lr=LR\n",
        ")\n",
        "\n",
        "model.train()\n",
        "losses = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_losses = []\n",
        "    random.shuffle(examples)\n",
        "\n",
        "    pbar = tqdm(range(0, len(examples), BATCH_SIZE), desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    for batch_idx in pbar:\n",
        "        batch_examples = examples[batch_idx:batch_idx + BATCH_SIZE]\n",
        "        batch_loss = 0.0\n",
        "\n",
        "        for ex in batch_examples:\n",
        "            prompt = ex[\"prompt\"]\n",
        "            chosen = ex[\"chosen\"]\n",
        "            rejected = ex[\"rejected\"]\n",
        "\n",
        "            # Prepare examples\n",
        "            c_ids, c_mask, c_start = prepare_example(tokenizer, prompt, chosen)\n",
        "            r_ids, r_mask, r_start = prepare_example(tokenizer, prompt, rejected)\n",
        "\n",
        "            c_ids, c_mask = c_ids.to(model.device), c_mask.to(model.device)\n",
        "            r_ids, r_mask = r_ids.to(model.device), r_mask.to(model.device)\n",
        "\n",
        "            # Forward pass\n",
        "            with torch.cuda.amp.autocast():\n",
        "                logp_chosen = get_sequence_logprob(model, c_ids, c_mask, c_start)\n",
        "                logp_rejected = get_sequence_logprob(model, r_ids, r_mask, r_start)\n",
        "\n",
        "                # Reference model logprobs\n",
        "                ref_logp_chosen = compute_ref_logprob(model, ref_lora_state, c_ids, c_mask, c_start)\n",
        "                ref_logp_rejected = compute_ref_logprob(model, ref_lora_state, r_ids, r_mask, r_start)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = dpo_apo_loss(logp_chosen, logp_rejected, ref_logp_chosen, ref_logp_rejected)\n",
        "                loss = loss.mean() / BATCH_SIZE\n",
        "\n",
        "            # Backward\n",
        "            loss.backward()\n",
        "            grad_norm = sum(p.grad.norm().item() for p in model.parameters() if p.grad is not None)\n",
        "            print(f\"Grad norm: {grad_norm:.6f}\")\n",
        "            batch_loss += loss.item() * BATCH_SIZE\n",
        "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "            # Clear cache\n",
        "            del c_ids, c_mask, r_ids, r_mask, logp_chosen, logp_rejected\n",
        "            del ref_logp_chosen, ref_logp_rejected, loss\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # Update weights after accumulation\n",
        "        if (batch_idx // BATCH_SIZE + 1) % ACCUMULATION_STEPS == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                [p for p in model.parameters() if p.requires_grad],\n",
        "                1.0\n",
        "            )\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        epoch_losses.append(batch_loss)\n",
        "        pbar.set_postfix({\"loss\": f\"{batch_loss:.4f}\"})\n",
        "\n",
        "    # Final update for remaining gradients\n",
        "    if len(epoch_losses) % ACCUMULATION_STEPS != 0:\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            [p for p in model.parameters() if p.requires_grad],\n",
        "            1.0\n",
        "        )\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "    losses.append(avg_loss)\n",
        "    print(f\"Epoch {epoch+1} avg loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "# %%\n",
        "# ### Save LoRA adapter weights\n",
        "# Note: This saves ONLY the LoRA weights, not the full model\n",
        "# Much smaller than full model checkpoint\n",
        "save_path = f\"lora_short_rank{LORA_RANK}\"\n",
        "model.save_pretrained(save_path)\n",
        "print(f\"Saved LoRA adapter to {save_path}/\")\n",
        "\n",
        "# Also save config for reference\n",
        "config_info = {\n",
        "    \"lora_rank\": LORA_RANK,\n",
        "    \"lora_alpha\": LORA_ALPHA,\n",
        "    \"target_modules\": TARGET_MODULES,\n",
        "    \"base_model\": MODEL_NAME,\n",
        "    \"lr\": LR,\n",
        "    \"epochs\": EPOCHS,\n",
        "    \"beta\": BETA,\n",
        "    \"training_examples\": len(examples),\n",
        "}\n",
        "\n",
        "with open(f\"{save_path}/training_config.json\", \"w\") as f:\n",
        "    json.dump(config_info, f, indent=2)\n",
        "\n",
        "print(f\"Saved training config to {save_path}/training_config.json\")\n",
        "\n",
        "# Download the adapter\n",
        "import shutil\n",
        "shutil.make_archive(save_path, 'zip', save_path)\n",
        "files.download(f\"{save_path}.zip\")\n",
        "\n",
        "# %%\n",
        "# ### Verify LoRA weights changed\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"VERIFICATION: Checking that LoRA weights updated\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "total_diff = 0.0\n",
        "num_params = 0\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if name in ref_lora_state:\n",
        "        diff = (param.data - ref_lora_state[name].to(param.device)).norm().item()\n",
        "        total_diff += diff\n",
        "        num_params += 1\n",
        "        if diff > 1e-6:\n",
        "            print(f\"  {name}: L2 diff = {diff:.6f} ✓\")\n",
        "        else:\n",
        "            print(f\"  {name}: unchanged ✗\")\n",
        "\n",
        "print(f\"\\nTotal L2 difference across all LoRA params: {total_diff:.6f}\")\n",
        "if total_diff > 1e-4:\n",
        "    print(\"✓ SUCCESS: LoRA parameters were updated during training\")\n",
        "else:\n",
        "    print(\"✗ WARNING: LoRA parameters may not have updated properly\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Loading saved LoRA weights later\n",
        "#\n",
        "# To load the saved LoRA adapter in a new session:\n",
        "# ```python\n",
        "# from peft import PeftModel\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "#\n",
        "# # Load base model\n",
        "# base_model = AutoModelForCausalLM.from_pretrained(\n",
        "#     \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "#     torch_dtype=torch.bfloat16,\n",
        "#     device_map=\"auto\",\n",
        "#     load_in_8bit=True,\n",
        "# )\n",
        "#\n",
        "# # Load LoRA adapter\n",
        "# model = PeftModel.from_pretrained(base_model, \"lora_short_rank8\")\n",
        "# ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc793c2a",
      "metadata": {
        "id": "dc793c2a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# %%\n",
        "# ### Plot training loss\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, EPOCHS+1), losses, 'b-o')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Loss')\n",
        "plt.title(f'LoRA (rank={LORA_RANK}) Training Loss')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b152a973",
      "metadata": {
        "id": "b152a973"
      },
      "outputs": [],
      "source": [
        "\n",
        "# %%\n",
        "# ### Test the fine-tuned model\n",
        "model.eval()\n",
        "\n",
        "test_prompts = [\n",
        "    \"What is machine learning?\",\n",
        "    \"Explain the theory of relativity.\",\n",
        "    \"What causes rain?\",\n",
        "    \"How do computers work?\",\n",
        "    \"What is the meaning of life?\",\n",
        "]\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(f\"TESTING LORA FINE-TUNED MODEL (rank={LORA_RANK})\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for p in test_prompts:\n",
        "    inputs = tokenizer(p, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=200,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    response = tokenizer.decode(out[0], skip_special_tokens=True)[len(p):]\n",
        "    word_count = len(response.split())\n",
        "    print(f\"\\nQ: {p}\")\n",
        "    print(f\"A: {response}\")\n",
        "    print(f\"[Word count: {word_count}]\")\n",
        "\n",
        "# %%\n",
        "# ### Compare with base model (optional - load fresh model)\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"COMPARISON: Loading base model for reference...\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eac0d76",
      "metadata": {
        "id": "1eac0d76"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Disable LoRA temporarily to see base model behavior\n",
        "model.disable_adapter_layers()\n",
        "\n",
        "for p in test_prompts[:2]:  # Just test first 2 to save time\n",
        "    inputs = tokenizer(p, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=200,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    response = tokenizer.decode(out[0], skip_special_tokens=True)[len(p):]\n",
        "    word_count = len(response.split())\n",
        "    print(f\"\\nQ: {p}\")\n",
        "    print(f\"A (BASE): {response}\")\n",
        "    print(f\"[Word count: {word_count}]\")\n",
        "\n",
        "# Re-enable LoRA\n",
        "model.enable_adapter_layers()\n",
        "print(\"\\nLoRA adapters re-enabled.\")\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d8fa81fb309f4da186b95f2208f5f72d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_712bb34b8dab410f8466d8be549ddcb6",
              "IPY_MODEL_4ba422a1e52e42b8b9401d8c3ae9bbe4",
              "IPY_MODEL_9056b2d8ee0b45f1b961940bc7470665"
            ],
            "layout": "IPY_MODEL_ed97905c0fd44e1c8302384b747b9fd7"
          }
        },
        "712bb34b8dab410f8466d8be549ddcb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10b46a0dfc2e4f888825cab89fe37e74",
            "placeholder": "​",
            "style": "IPY_MODEL_1358fb65548c42a480a026134e3b955d",
            "value": "tokenizer_config.json: "
          }
        },
        "4ba422a1e52e42b8b9401d8c3ae9bbe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0b7f5643d2b41dfbb8cb72669824b2c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b0fabb8648e49cbae5a4fb45614033b",
            "value": 1
          }
        },
        "9056b2d8ee0b45f1b961940bc7470665": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc83de7371f648d7a25eebe3c437eff8",
            "placeholder": "​",
            "style": "IPY_MODEL_4df42dbb1173470cb5b1e455d910dfaf",
            "value": " 2.10k/? [00:00&lt;00:00, 237kB/s]"
          }
        },
        "ed97905c0fd44e1c8302384b747b9fd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10b46a0dfc2e4f888825cab89fe37e74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1358fb65548c42a480a026134e3b955d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0b7f5643d2b41dfbb8cb72669824b2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "9b0fabb8648e49cbae5a4fb45614033b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc83de7371f648d7a25eebe3c437eff8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4df42dbb1173470cb5b1e455d910dfaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "322d77ac1f9b4fd18bda8aac82bb9a6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62fc65648e08482d802b8d47600f41c9",
              "IPY_MODEL_e0206ab6ad5647609583c617986c60cf",
              "IPY_MODEL_73d097622ee545f5b6f0de20e42ee8d1"
            ],
            "layout": "IPY_MODEL_70e149e381474fa3b835b0faeff37586"
          }
        },
        "62fc65648e08482d802b8d47600f41c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cd031a30a84442ba562b1e67eac2b00",
            "placeholder": "​",
            "style": "IPY_MODEL_11567ae394ac42988cfcdc80b06c22aa",
            "value": "tokenizer.model:   0%"
          }
        },
        "e0206ab6ad5647609583c617986c60cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a02a9d97ca5845d783271d93575d899d",
            "max": 493443,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_82e70b5c07ec4ac49388cc0976004bf5",
            "value": 0
          }
        },
        "73d097622ee545f5b6f0de20e42ee8d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6823909d8d1450283dda5452a120f7c",
            "placeholder": "​",
            "style": "IPY_MODEL_533a972919424521891f260dbcdc4430",
            "value": " 0.00/493k [00:00&lt;?, ?B/s]"
          }
        },
        "70e149e381474fa3b835b0faeff37586": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cd031a30a84442ba562b1e67eac2b00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11567ae394ac42988cfcdc80b06c22aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a02a9d97ca5845d783271d93575d899d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82e70b5c07ec4ac49388cc0976004bf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e6823909d8d1450283dda5452a120f7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "533a972919424521891f260dbcdc4430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}