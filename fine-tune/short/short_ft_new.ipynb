{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91d3a4df",
   "metadata": {},
   "source": [
    "# LoRA Fine-Tuning with APO-up Loss\n",
    "\n",
    "Fine-tuning Mistral-7B-Instruct-v0.2 using:\n",
    "- **LoRA** (rank-8) on `q_proj, v_proj`\n",
    "- **APO-up loss** (Anchored Preference Optimization)\n",
    "- **8-bit quantization** for memory efficiency (~24GB GPU)\n",
    "\n",
    "Dataset: `short_ft.jsonl` with \"prompt\", \"chosen\", \"rejected\" fields.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b482ef",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733464cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "LORA_RANK = 8  # Change this to adjust LoRA rank for fine-tuning\n",
    "\n",
    "!pip install -q transformers accelerate bitsandbytes torch peft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917ec762",
   "metadata": {},
   "source": [
    "## 2. Upload Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5259a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import json\n",
    "\n",
    "# Upload short_ft.jsonl\n",
    "print(\"Please upload short_ft.jsonl\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Load dataset\n",
    "examples = []\n",
    "with open(\"short_ft.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            examples.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(examples)} examples.\")\n",
    "print(f\"First example:\")\n",
    "print(f\"  Prompt: {examples[0]['prompt'][:100]}...\")\n",
    "print(f\"  Chosen: {examples[0]['chosen'][:100]}...\")\n",
    "print(f\"  Rejected: {examples[0]['rejected'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f07bf3",
   "metadata": {},
   "source": [
    "## 3. Load Model with 8-bit Quantization + LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7422f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# 8-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model in 8-bit\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training (required for QLoRA)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA config: on q_proj, v_proj (minimal, comparable to targeted training)\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_RANK,  # rank (set at top of notebook)\n",
    "    lora_alpha=LORA_RANK * 2,  # scaling factor (typically 2x rank)\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(f\"Model loaded with LoRA adapters.\")\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e9dad0",
   "metadata": {},
   "source": [
    "## 4. APO-up Loss Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7aa2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "BETA = 0.2\n",
    "\n",
    "def get_sequence_logprob(model, input_ids, attention_mask, response_start_idx):\n",
    "    \"\"\"\n",
    "    Compute log probability of the response portion of a sequence.\n",
    "    response_start_idx: index where the response tokens begin\n",
    "    \"\"\"\n",
    "    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # [batch, seq_len, vocab]\n",
    "\n",
    "    # Shift for next-token prediction\n",
    "    shift_logits = logits[:, :-1, :].contiguous()\n",
    "    shift_labels = input_ids[:, 1:].contiguous()\n",
    "\n",
    "    # Compute log probs\n",
    "    log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "    token_log_probs = torch.gather(log_probs, dim=-1, index=shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    # Mask: only count response tokens (after response_start_idx)\n",
    "    response_mask = torch.zeros_like(token_log_probs)\n",
    "    response_mask[:, response_start_idx-1:] = attention_mask[:, response_start_idx:]\n",
    "\n",
    "    # Sum log probs over response\n",
    "    seq_log_prob = (token_log_probs * response_mask).sum(dim=-1)\n",
    "    return seq_log_prob\n",
    "\n",
    "\n",
    "def compute_ref_logprob(model, input_ids, attention_mask, response_start_idx):\n",
    "    \"\"\"\n",
    "    Compute logprob using reference model (LoRA adapters disabled).\n",
    "    \"\"\"\n",
    "    # Disable LoRA adapters to get reference (base model) behavior\n",
    "    model.disable_adapter_layers()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ref_logprob = get_sequence_logprob(model, input_ids, attention_mask, response_start_idx)\n",
    "    \n",
    "    # Re-enable LoRA adapters\n",
    "    model.enable_adapter_layers()\n",
    "    \n",
    "    return ref_logprob\n",
    "\n",
    "\n",
    "def dpo_apo_loss(logp_chosen, logp_rejected, ref_logp_chosen, ref_logp_rejected, beta=BETA):\n",
    "    \"\"\"\n",
    "    DPO + APO-up loss:\n",
    "    t1 = -log(sigmoid(beta * (logp_c - logp_r - (ref_logp_c - ref_logp_r))))  [DPO term]\n",
    "    t2 = -log(sigmoid(beta * (logp_c - ref_logp_c)))  [APO-up term: anchor chosen likelihood]\n",
    "    \"\"\"\n",
    "    # DPO term\n",
    "    logit_diff = logp_chosen - logp_rejected - (ref_logp_chosen - ref_logp_rejected)\n",
    "    t1 = -F.logsigmoid(beta * logit_diff)\n",
    "\n",
    "    # APO-up term (anchoring chosen likelihood)\n",
    "    t2 = -F.logsigmoid(beta * (logp_chosen - ref_logp_chosen))\n",
    "\n",
    "    return t1 + t2\n",
    "\n",
    "print(\"APO-up loss functions defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb00037",
   "metadata": {},
   "source": [
    "## 5. Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0c6152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_example(tokenizer, prompt, response, max_length=1024):\n",
    "    \"\"\"\n",
    "    Tokenize prompt + response, return input_ids, attention_mask, and response start index.\n",
    "    \"\"\"\n",
    "    # Tokenize prompt separately to find where response starts\n",
    "    prompt_ids = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "    response_ids = tokenizer.encode(response, add_special_tokens=False)\n",
    "\n",
    "    # Combine\n",
    "    full_ids = prompt_ids + response_ids\n",
    "\n",
    "    # Truncate if needed\n",
    "    if len(full_ids) > max_length:\n",
    "        full_ids = full_ids[:max_length]\n",
    "\n",
    "    response_start_idx = len(prompt_ids)\n",
    "\n",
    "    input_ids = torch.tensor([full_ids])\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "    return input_ids, attention_mask, response_start_idx\n",
    "\n",
    "print(\"Data utilities defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68794013",
   "metadata": {},
   "source": [
    "## 6. Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fd8832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Hyperparameters\n",
    "LR = 1e-4\n",
    "EPOCHS = 5\n",
    "MAX_LENGTH = 1024\n",
    "BATCH_SIZE = 1\n",
    "ACCUMULATION_STEPS = 10  # Effective batch size = 1 * 10 = 10\n",
    "\n",
    "# Create checkpoint directory\n",
    "CHECKPOINT_DIR = \"lora_checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Optimizer - only train LoRA parameters\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)\n",
    "\n",
    "# Training\n",
    "model.train()\n",
    "losses = []\n",
    "all_epoch_losses = []\n",
    "\n",
    "# === Peak Memory Tracking ===\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "peak_memory_per_epoch = []\n",
    "\n",
    "print(f\"Starting training...\")\n",
    "print(f\"  Examples: {len(examples)}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Accumulation steps: {ACCUMULATION_STEPS}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * ACCUMULATION_STEPS}\")\n",
    "print(f\"  Learning rate: {LR}\")\n",
    "print(f\"  Beta (APO): {BETA}\")\n",
    "print()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_losses = []\n",
    "    random.shuffle(examples)\n",
    "\n",
    "    pbar = tqdm(range(0, len(examples), BATCH_SIZE), desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for step, batch_idx in enumerate(pbar):\n",
    "        batch_examples = examples[batch_idx:batch_idx + BATCH_SIZE]\n",
    "        batch_loss = 0.0\n",
    "\n",
    "        for ex in batch_examples:\n",
    "            prompt = ex[\"prompt\"]\n",
    "            chosen = ex[\"chosen\"]\n",
    "            rejected = ex[\"rejected\"]\n",
    "\n",
    "            # Prepare examples\n",
    "            c_ids, c_mask, c_start = prepare_example(tokenizer, prompt, chosen, MAX_LENGTH)\n",
    "            r_ids, r_mask, r_start = prepare_example(tokenizer, prompt, rejected, MAX_LENGTH)\n",
    "\n",
    "            c_ids, c_mask = c_ids.to(model.device), c_mask.to(model.device)\n",
    "            r_ids, r_mask = r_ids.to(model.device), r_mask.to(model.device)\n",
    "\n",
    "            # Forward pass\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                logp_chosen = get_sequence_logprob(model, c_ids, c_mask, c_start)\n",
    "                logp_rejected = get_sequence_logprob(model, r_ids, r_mask, r_start)\n",
    "\n",
    "                # Reference model logprobs (LoRA disabled)\n",
    "                ref_logp_chosen = compute_ref_logprob(model, c_ids, c_mask, c_start)\n",
    "                ref_logp_rejected = compute_ref_logprob(model, r_ids, r_mask, r_start)\n",
    "\n",
    "                # Compute APO-up loss\n",
    "                loss = dpo_apo_loss(logp_chosen, logp_rejected, ref_logp_chosen, ref_logp_rejected)\n",
    "                loss = loss.mean() / ACCUMULATION_STEPS  # Scale for gradient accumulation\n",
    "\n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            batch_loss += loss.item() * ACCUMULATION_STEPS\n",
    "\n",
    "            # Clear cache\n",
    "            del c_ids, c_mask, r_ids, r_mask, logp_chosen, logp_rejected\n",
    "            del ref_logp_chosen, ref_logp_rejected, loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        epoch_losses.append(batch_loss)\n",
    "        pbar.set_postfix({\"loss\": f\"{batch_loss:.4f}\"})\n",
    "\n",
    "        # Update weights after accumulating\n",
    "        if (step + 1) % ACCUMULATION_STEPS == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    # Final update if remaining gradients\n",
    "    if len(epoch_losses) % ACCUMULATION_STEPS != 0:\n",
    "        torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    losses.append(avg_loss)\n",
    "    all_epoch_losses.append(epoch_losses)\n",
    "    \n",
    "    # Track peak memory for this epoch\n",
    "    epoch_peak_memory = torch.cuda.max_memory_allocated() / (1024 ** 3)  # Convert to GB\n",
    "    peak_memory_per_epoch.append(epoch_peak_memory)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} avg loss: {avg_loss:.4f} | Peak GPU Memory: {epoch_peak_memory:.2f} GB\")\n",
    "\n",
    "    # Save checkpoint after each epoch\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"epoch_{epoch+1}\")\n",
    "    model.save_pretrained(checkpoint_path)\n",
    "    print(f\"  Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "# Final peak memory measurement\n",
    "final_peak_memory_gb = torch.cuda.max_memory_allocated() / (1024 ** 3)\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Peak GPU Memory Usage: {final_peak_memory_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809cfdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot epoch averages\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, EPOCHS+1), losses, 'b-o', linewidth=2, markersize=8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.title('Training Loss per Epoch')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot all step losses (flattened)\n",
    "plt.subplot(1, 2, 2)\n",
    "all_losses_flat = [loss for epoch_losses in all_epoch_losses for loss in epoch_losses]\n",
    "plt.plot(all_losses_flat, 'b-', alpha=0.5, linewidth=0.5)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss per Step')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_loss.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Loss plot saved to training_loss.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d92fcb",
   "metadata": {},
   "source": [
    "## 8. Save and Download Trained LoRA Adapters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5e13a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Save final LoRA adapters\n",
    "FINAL_ADAPTER_DIR = f\"short_ft_lora_{LORA_RANK}\"\n",
    "model.save_pretrained(FINAL_ADAPTER_DIR)\n",
    "tokenizer.save_pretrained(FINAL_ADAPTER_DIR)\n",
    "\n",
    "print(f\"Final LoRA adapters saved to {FINAL_ADAPTER_DIR}/\")\n",
    "print(f\"\\nFiles saved:\")\n",
    "for f in os.listdir(FINAL_ADAPTER_DIR):\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "# Also save training metadata\n",
    "import json\n",
    "metadata = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"lora_rank\": LORA_RANK,\n",
    "    \"lora_alpha\": LORA_RANK * 2,\n",
    "    \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
    "    \"learning_rate\": LR,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"accumulation_steps\": ACCUMULATION_STEPS,\n",
    "    \"beta\": BETA,\n",
    "    \"max_length\": MAX_LENGTH,\n",
    "    \"num_examples\": len(examples),\n",
    "    \"final_avg_loss\": losses[-1] if losses else None,\n",
    "    \"epoch_losses\": losses,\n",
    "    \"peak_memory_gb\": final_peak_memory_gb,\n",
    "    \"peak_memory_per_epoch_gb\": peak_memory_per_epoch,\n",
    "}\n",
    "\n",
    "with open(os.path.join(FINAL_ADAPTER_DIR, \"training_metadata.json\"), \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nTraining metadata saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429a59a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the adapter directory for easy download\n",
    "shutil.make_archive(\"short_ft_lora_final\", 'zip', FINAL_ADAPTER_DIR)\n",
    "print(\"Created short_ft_lora_final.zip\")\n",
    "\n",
    "# Download the zip file\n",
    "from google.colab import files\n",
    "files.download(\"short_ft_lora_final.zip\")\n",
    "\n",
    "# Also download the loss plot\n",
    "files.download(\"training_loss.png\")\n",
    "\n",
    "print(\"\\nDownload complete! Files downloaded:\")\n",
    "print(\"  - short_ft_lora_final.zip (LoRA adapters + tokenizer + metadata)\")\n",
    "print(\"  - training_loss.png (training loss plot)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97068bd8",
   "metadata": {},
   "source": [
    "## 9. Quick Inference Test (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7aca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test to verify training worked\n",
    "model.eval()\n",
    "\n",
    "test_prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain how neural networks work.\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INFERENCE TEST (with trained LoRA)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"\\nQ: {prompt}\")\n",
    "    print(f\"A: {response[len(prompt):]}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
