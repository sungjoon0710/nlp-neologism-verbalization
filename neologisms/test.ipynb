{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Neologism Embedding Analysis: ~short and ~kidmode\n",
    "\n",
    "This notebook loads the trained neologism embeddings and asks the model what each token means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers accelerate bitsandbytes torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with 8-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Original vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load neologism embeddings from .pt files\n",
    "SHORT_EMBEDDING_PATH = \"short.pt\"\n",
    "KIDMODE_EMBEDDING_PATH = \"kidmode.pt\"\n",
    "\n",
    "short_data = torch.load(SHORT_EMBEDDING_PATH, map_location=\"cpu\", weights_only=False)\n",
    "kidmode_data = torch.load(KIDMODE_EMBEDDING_PATH, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SHORT EMBEDDING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Neologism: {short_data['neologism']}\")\n",
    "print(f\"  Embedding shape: {short_data['embedding'].shape}\")\n",
    "print(f\"  Initialized from: '{short_data['init_word']}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KIDMODE EMBEDDING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Neologism: {kidmode_data['neologism']}\")\n",
    "print(f\"  Embedding shape: {kidmode_data['embedding'].shape}\")\n",
    "print(f\"  Initialized from: '{kidmode_data['init_word']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add-tokens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract neologism tokens and embeddings\n",
    "NEOLOGISM_SHORT = short_data['neologism']\n",
    "NEOLOGISM_KIDMODE = kidmode_data['neologism']\n",
    "short_embedding = short_data['embedding']\n",
    "kidmode_embedding = kidmode_data['embedding']\n",
    "\n",
    "# Add both neologism tokens to tokenizer\n",
    "num_added = tokenizer.add_tokens([NEOLOGISM_SHORT, NEOLOGISM_KIDMODE])\n",
    "print(f\"Added {num_added} new token(s) to vocabulary\")\n",
    "\n",
    "# Get the new token IDs\n",
    "short_id = tokenizer.convert_tokens_to_ids(NEOLOGISM_SHORT)\n",
    "kidmode_id = tokenizer.convert_tokens_to_ids(NEOLOGISM_KIDMODE)\n",
    "print(f\"New token '{NEOLOGISM_SHORT}' assigned ID: {short_id}\")\n",
    "print(f\"New token '{NEOLOGISM_KIDMODE}' assigned ID: {kidmode_id}\")\n",
    "\n",
    "# Resize model embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"Resized model embeddings. New vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inject-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inject learned embeddings into the model\n",
    "device = model.model.embed_tokens.weight.device\n",
    "dtype = model.model.embed_tokens.weight.dtype\n",
    "\n",
    "short_embedding_tensor = short_embedding.to(device=device, dtype=dtype)\n",
    "kidmode_embedding_tensor = kidmode_embedding.to(device=device, dtype=dtype)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.model.embed_tokens.weight[short_id] = short_embedding_tensor\n",
    "    model.model.embed_tokens.weight[kidmode_id] = kidmode_embedding_tensor\n",
    "\n",
    "print(f\"Injected learned embedding for '{NEOLOGISM_SHORT}'\")\n",
    "print(f\"  Embedding L2 norm: {model.model.embed_tokens.weight[short_id].norm().item():.4f}\")\n",
    "\n",
    "print(f\"\\nInjected learned embedding for '{NEOLOGISM_KIDMODE}'\")\n",
    "print(f\"  Embedding L2 norm: {model.model.embed_tokens.weight[kidmode_id].norm().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ask-meaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the model what each neologism means\n",
    "model.eval()\n",
    "\n",
    "prompts = [\n",
    "    f\"What does {NEOLOGISM_SHORT} mean?\",\n",
    "    f\"What does {NEOLOGISM_KIDMODE} mean?\",\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ASKING THE MODEL WHAT EACH NEOLOGISM MEANS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for prompt in prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=500,\n",
    "            do_sample=True,\n",
    "            temperature=0.3,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):].strip()\n",
    "    print(f\"\\nQ: {prompt}\")\n",
    "    print(f\"A: {response}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vector-mean",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the vector mean of both embeddings\n",
    "print(\"=\"*80)\n",
    "print(\"VECTOR MEAN OF BOTH EMBEDDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compute mean on CPU for consistency\n",
    "short_emb = short_data['embedding'].float()\n",
    "kidmode_emb = kidmode_data['embedding'].float()\n",
    "\n",
    "vector_mean = (short_emb + kidmode_emb) / 2\n",
    "\n",
    "print(f\"\\n~short embedding shape: {short_emb.shape}\")\n",
    "print(f\"~kidmode embedding shape: {kidmode_emb.shape}\")\n",
    "print(f\"Vector mean shape: {vector_mean.shape}\")\n",
    "\n",
    "print(f\"\\n~short L2 norm: {short_emb.norm().item():.4f}\")\n",
    "print(f\"~kidmode L2 norm: {kidmode_emb.norm().item():.4f}\")\n",
    "print(f\"Vector mean L2 norm: {vector_mean.norm().item():.4f}\")\n",
    "\n",
    "# Cosine similarity between the two embeddings\n",
    "cosine_sim = torch.nn.functional.cosine_similarity(short_emb.unsqueeze(0), kidmode_emb.unsqueeze(0)).item()\n",
    "print(f\"\\nCosine similarity between ~short and ~kidmode: {cosine_sim:.4f}\")\n",
    "\n",
    "print(f\"\\nVector mean (first 20 dimensions):\")\n",
    "print(vector_mean[:20])\n",
    "\n",
    "print(f\"\\nVector mean (full tensor):\")\n",
    "print(vector_mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
