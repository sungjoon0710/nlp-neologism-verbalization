{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d35aa4b",
   "metadata": {},
   "source": [
    "# Neologism Composition Inference: `~short` + `~kidmode` on Mistral-7B\n",
    "\n",
    "This notebook demonstrates how to load **multiple** pre-trained neologism embeddings and compose them for inference on Mistral-7B.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Load the base Mistral-7B-Instruct-v0.2 model and tokenizer\n",
    "2. Load both saved neologism embeddings (`kidmode.pt` and `short.pt`)\n",
    "3. Add both `~kidmode` and `~short` tokens to the vocabulary\n",
    "4. Resize model embeddings and inject both learned embeddings\n",
    "5. Run inference with the composed tokens (e.g., \"Give me a ~short ~kidmode answer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a6f520",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70896fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers accelerate bitsandbytes torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ca274",
   "metadata": {},
   "source": [
    "## Step 2: Load Base Model and Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055d4fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with 8-bit quantization for memory efficiency\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Original vocab size: {len(tokenizer)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a9968b",
   "metadata": {},
   "source": [
    "## Step 3: Load Both Neologism Embeddings\n",
    "\n",
    "Load both `~kidmode` and `~short` embeddings from their saved `.pt` files.\n",
    "\n",
    "Each embedding was trained using DPO + APO-up loss and saved with the following structure:\n",
    "- `neologism`: The token string (e.g., \"~kidmode\" or \"~short\")\n",
    "- `token_id`: The assigned token ID (32000)\n",
    "- `embedding`: The learned embedding tensor (shape: [4096])\n",
    "- `init_word`: The word used for initialization (\"general\")\n",
    "- `model_name`: The base model name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Paths to embedding files\n",
    "# Update these paths as needed (relative to this notebook or absolute paths)\n",
    "KIDMODE_EMBEDDING_PATH = \"kidmode.pt\"\n",
    "SHORT_EMBEDDING_PATH = \"short.pt\"\n",
    "\n",
    "# Check if embedding files exist\n",
    "def check_and_upload_embedding(path, name):\n",
    "    \"\"\"Check if embedding file exists, prompt upload if in Colab.\"\"\"\n",
    "    if os.path.exists(path):\n",
    "        print(f\"Found {name} embedding file: {path}\")\n",
    "        return path\n",
    "    else:\n",
    "        print(f\"{name} embedding file not found: {path}\")\n",
    "        print(\"Attempting to upload...\")\n",
    "        \n",
    "        # Try Google Colab upload\n",
    "        try:\n",
    "            from google.colab import files\n",
    "            uploaded = files.upload()\n",
    "            if uploaded:\n",
    "                uploaded_path = list(uploaded.keys())[0]\n",
    "                print(f\"Uploaded: {uploaded_path}\")\n",
    "                return uploaded_path\n",
    "        except ImportError:\n",
    "            print(f\"\\nNot running in Google Colab.\")\n",
    "            print(f\"Please ensure the embedding file is in the current directory:\")\n",
    "            print(f\"  Expected path: {os.path.abspath(path)}\")\n",
    "            raise FileNotFoundError(f\"Please place '{path}' in the working directory and re-run this cell.\")\n",
    "    return path\n",
    "\n",
    "# Load kidmode embedding\n",
    "KIDMODE_EMBEDDING_PATH = check_and_upload_embedding(KIDMODE_EMBEDDING_PATH, \"kidmode\")\n",
    "kidmode_data = torch.load(KIDMODE_EMBEDDING_PATH, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KIDMODE EMBEDDING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Neologism: {kidmode_data['neologism']}\")\n",
    "print(f\"  Original token ID: {kidmode_data['token_id']}\")\n",
    "print(f\"  Embedding shape: {kidmode_data['embedding'].shape}\")\n",
    "print(f\"  Initialized from: '{kidmode_data['init_word']}'\")\n",
    "print(f\"  Model: {kidmode_data['model_name']}\")\n",
    "\n",
    "# Load short embedding\n",
    "SHORT_EMBEDDING_PATH = check_and_upload_embedding(SHORT_EMBEDDING_PATH, \"short\")\n",
    "short_data = torch.load(SHORT_EMBEDDING_PATH, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SHORT EMBEDDING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Neologism: {short_data['neologism']}\")\n",
    "print(f\"  Original token ID: {short_data['token_id']}\")\n",
    "print(f\"  Embedding shape: {short_data['embedding'].shape}\")\n",
    "print(f\"  Initialized from: '{short_data['init_word']}'\")\n",
    "print(f\"  Model: {short_data['model_name']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b7b5e0",
   "metadata": {},
   "source": [
    "## Step 4: Add Both Neologism Tokens to Vocabulary\n",
    "\n",
    "Here we:\n",
    "1. Add both `~kidmode` and `~short` tokens to the tokenizer\n",
    "2. Resize the model's embedding layer once to accommodate both new tokens\n",
    "3. Replace the randomly initialized embeddings with our learned embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b6260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract neologism tokens and embeddings\n",
    "NEOLOGISM_KIDMODE = kidmode_data['neologism']\n",
    "NEOLOGISM_SHORT = short_data['neologism']\n",
    "kidmode_embedding = kidmode_data['embedding']\n",
    "short_embedding = short_data['embedding']\n",
    "\n",
    "# Add both neologism tokens to tokenizer at once\n",
    "num_added = tokenizer.add_tokens([NEOLOGISM_KIDMODE, NEOLOGISM_SHORT])\n",
    "print(f\"Added {num_added} new token(s) to vocabulary\")\n",
    "\n",
    "# Get the new token IDs\n",
    "kidmode_id = tokenizer.convert_tokens_to_ids(NEOLOGISM_KIDMODE)\n",
    "short_id = tokenizer.convert_tokens_to_ids(NEOLOGISM_SHORT)\n",
    "print(f\"New token '{NEOLOGISM_KIDMODE}' assigned ID: {kidmode_id}\")\n",
    "print(f\"New token '{NEOLOGISM_SHORT}' assigned ID: {short_id}\")\n",
    "\n",
    "# Resize model embeddings to include the new tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"Resized model embeddings. New vocab size: {len(tokenizer)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab7464a",
   "metadata": {},
   "source": [
    "## Step 5: Inject Both Learned Embeddings\n",
    "\n",
    "Replace the default (randomly initialized) embeddings for `~kidmode` and `~short` with our trained embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfca922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move embeddings to correct device and dtype\n",
    "device = model.model.embed_tokens.weight.device\n",
    "dtype = model.model.embed_tokens.weight.dtype\n",
    "\n",
    "kidmode_embedding_tensor = kidmode_embedding.to(device=device, dtype=dtype)\n",
    "short_embedding_tensor = short_embedding.to(device=device, dtype=dtype)\n",
    "\n",
    "# Inject both learned embeddings\n",
    "with torch.no_grad():\n",
    "    model.model.embed_tokens.weight[kidmode_id] = kidmode_embedding_tensor\n",
    "    model.model.embed_tokens.weight[short_id] = short_embedding_tensor\n",
    "\n",
    "print(f\"Successfully injected learned embedding for '{NEOLOGISM_KIDMODE}'\")\n",
    "print(f\"  Embedding L2 norm: {model.model.embed_tokens.weight[kidmode_id].norm().item():.4f}\")\n",
    "\n",
    "print(f\"\\nSuccessfully injected learned embedding for '{NEOLOGISM_SHORT}'\")\n",
    "print(f\"  Embedding L2 norm: {model.model.embed_tokens.weight[short_id].norm().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50f6dc4",
   "metadata": {},
   "source": [
    "## Step 6: Verify Token Integration\n",
    "\n",
    "Confirm that the tokenizer correctly recognizes and encodes both neologisms, including when they are composed together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6ab03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tokenization with individual tokens\n",
    "test_texts = [\n",
    "    f\"What is machine learning? Give me a {NEOLOGISM_KIDMODE} answer.\",\n",
    "    f\"What is machine learning? Give me a {NEOLOGISM_SHORT} answer.\",\n",
    "    f\"What is machine learning? Give me a {NEOLOGISM_SHORT} {NEOLOGISM_KIDMODE} answer.\",\n",
    "]\n",
    "\n",
    "for test_text in test_texts:\n",
    "    tokens = tokenizer.tokenize(test_text)\n",
    "    token_ids = tokenizer.encode(test_text, add_special_tokens=False)\n",
    "    \n",
    "    print(f\"\\nTest text: '{test_text}'\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "    print(f\"'{NEOLOGISM_KIDMODE}' recognized: {NEOLOGISM_KIDMODE in tokens}\")\n",
    "    print(f\"'{NEOLOGISM_SHORT}' recognized: {NEOLOGISM_SHORT in tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62ca9f1",
   "metadata": {},
   "source": [
    "## Step 7: Load Test Prompts from LIMA\n",
    "\n",
    "Dataset from LIMA for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17b695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "HF_TOKEN = \"\"  # Add your HF authentication token here\n",
    "login(token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2948f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "lima_test_dataset = load_dataset(\"GAIR/lima\", split=\"test\", revision=\"refs/convert/parquet\")\n",
    "print(f\"Loaded {len(lima_test_dataset)} test examples from LIMA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72f2985",
   "metadata": {},
   "source": [
    "## Step 8: Run Sanity Check Inference\n",
    "\n",
    "Test the model with individual neologisms and composed neologisms to verify everything works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754dc1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Test prompts: individual neologisms and composed\n",
    "sanity_prompts = [\n",
    "    # Individual tokens\n",
    "    f\"What is a synonym for {NEOLOGISM_KIDMODE}? Just provide a list of 5 synonyms, no elaboration\",\n",
    "    f\"What is a synonym for {NEOLOGISM_SHORT}? Just provide a list of 5 synonyms, no elaboration\",\n",
    "    # Individual usage\n",
    "    f\"What is machine learning? Give me a {NEOLOGISM_KIDMODE} answer\",\n",
    "    f\"What is machine learning? Give me a {NEOLOGISM_SHORT} answer\",\n",
    "    # COMPOSED usage - the main point of this notebook\n",
    "    f\"What is machine learning? Give me a {NEOLOGISM_SHORT} {NEOLOGISM_KIDMODE} answer\",\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SANITY CHECK: Individual and Composed Neologism Inference\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for p in sanity_prompts:\n",
    "    inputs = tokenizer(p, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=500,\n",
    "            do_sample=True,\n",
    "            temperature=0.3,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    response = tokenizer.decode(out[0], skip_special_tokens=True)[len(p):].strip()\n",
    "    print(f\"\\nQ: {p}\")\n",
    "    print(f\"A: {response[:500]}...\")\n",
    "    print(\"-\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dfdb4d",
   "metadata": {},
   "source": [
    "## Step 9: Run Full Inference on LIMA with Composed Neologisms\n",
    "\n",
    "Run inference on all LIMA test prompts using the composed `~short ~kidmode` neologisms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908870bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Output file path\n",
    "OUTPUT_PATH = \"composition_inference_results.jsonl\"\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "print(f\"Processing {len(lima_test_dataset)} examples with composed neologisms (~short ~kidmode)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for idx, example in enumerate(tqdm(lima_test_dataset, desc=\"Generating responses\")):\n",
    "    # Extract the question from conversations\n",
    "    conversations = example['conversations']\n",
    "\n",
    "    # Get the first message (the question)\n",
    "    if isinstance(conversations, list) and len(conversations) > 0:\n",
    "        question = conversations[0]\n",
    "    else:\n",
    "        question = str(conversations)\n",
    "\n",
    "    # Create prompt with composed neologisms (~short ~kidmode)\n",
    "    prompt = f\"{question} Give me a {NEOLOGISM_SHORT}{NEOLOGISM_KIDMODE} answer.\"\n",
    "\n",
    "    # Tokenize and generate\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2000,\n",
    "            do_sample=True,\n",
    "            temperature=0.3,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode response\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = full_response[len(prompt):].strip()\n",
    "\n",
    "    # Create result entry\n",
    "    result = {\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": response,\n",
    "        \"neologisms_used\": [NEOLOGISM_SHORT, NEOLOGISM_KIDMODE]\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    # Print progress every 50 examples\n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"\\nExample {idx + 1}:\")\n",
    "        print(f\"  Q: {prompt[:80]}...\")\n",
    "        print(f\"  A: {response[:100]}...\")\n",
    "\n",
    "# Save results to JSONL\n",
    "with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:\n",
    "    for result in results:\n",
    "        f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Saved {len(results)} results to {OUTPUT_PATH}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Download file in Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(OUTPUT_PATH)\n",
    "    print(f\"Downloading {OUTPUT_PATH}...\")\n",
    "except ImportError:\n",
    "    print(f\"Not in Colab. File saved locally at: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6abf8cf",
   "metadata": {},
   "source": [
    "## Step 10: Response Length Analysis\n",
    "\n",
    "Compute summary statistics and visualize response length distributions for the composed neologism responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b70a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load results from JSONL (if running separately)\n",
    "RESULTS_PATH = \"composition_inference_results.jsonl\"\n",
    "\n",
    "if 'results' not in dir() or len(results) == 0:\n",
    "    results = []\n",
    "    with open(RESULTS_PATH, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            results.append(json.loads(line))\n",
    "\n",
    "print(f\"Analyzing {len(results)} responses\")\n",
    "\n",
    "# Calculate word lengths and token lengths\n",
    "word_lengths = []\n",
    "token_lengths = []\n",
    "\n",
    "for r in results:\n",
    "    response = r['response']\n",
    "    \n",
    "    # Word count (split by whitespace)\n",
    "    words = response.split()\n",
    "    word_lengths.append(len(words))\n",
    "    \n",
    "    # Token count (using tokenizer)\n",
    "    tokens = tokenizer.encode(response, add_special_tokens=False)\n",
    "    token_lengths.append(len(tokens))\n",
    "\n",
    "word_lengths = np.array(word_lengths)\n",
    "token_lengths = np.array(token_lengths)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPOSED NEOLOGISM (~short ~kidmode) RESPONSE LENGTH STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nWord Length:\")\n",
    "print(f\"  Mean:   {word_lengths.mean():.2f}\")\n",
    "print(f\"  Stdev:  {word_lengths.std():.2f}\")\n",
    "print(f\"  Min:    {word_lengths.min()}\")\n",
    "print(f\"  Max:    {word_lengths.max()}\")\n",
    "\n",
    "print(f\"\\nToken Length:\")\n",
    "print(f\"  Mean:   {token_lengths.mean():.2f}\")\n",
    "print(f\"  Stdev:  {token_lengths.std():.2f}\")\n",
    "print(f\"  Min:    {token_lengths.min()}\")\n",
    "print(f\"  Max:    {token_lengths.max()}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Histograms\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Word length histogram\n",
    "axes[0].hist(word_lengths, bins=30, edgecolor='black', alpha=0.7, color='mediumseagreen')\n",
    "axes[0].axvline(word_lengths.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {word_lengths.mean():.1f}')\n",
    "axes[0].set_xlabel('Word Count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Composed (~short ~kidmode) Response Length (Words)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Token length histogram\n",
    "axes[1].hist(token_lengths, bins=30, edgecolor='black', alpha=0.7, color='mediumpurple')\n",
    "axes[1].axvline(token_lengths.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {token_lengths.mean():.1f}')\n",
    "axes[1].set_xlabel('Token Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Composed (~short ~kidmode) Response Length (Tokens)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('composition_response_length_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSaved visualization to 'composition_response_length_analysis.png'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9586159b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
