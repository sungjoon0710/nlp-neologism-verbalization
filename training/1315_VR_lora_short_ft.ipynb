{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LoRA Fine-Tuning for \"Short Response\" Concept\n",
        "## Mistral-7B-Instruct-v0.2\n",
        "\n",
        "This notebook uses HuggingFace TRL's DPOTrainer for proper DPO training.\n",
        "\n",
        "**Key improvements over manual implementation:**\n",
        "1. Uses 4-bit quantization with proper QLoRA setup (NOT 8-bit)\n",
        "2. Proper DPO training with reference model handling\n",
        "3. Higher learning rate appropriate for LoRA\n",
        "4. Gradient checkpointing for memory efficiency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1: Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install -q torch\n",
        "%pip install -q bitsandbytes accelerate\n",
        "%pip install -q transformers peft datasets\n",
        "%pip install -q trl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: Configuration & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\varun\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "  LoRA Rank: 8\n",
            "  Learning Rate: 0.0002\n",
            "  Epochs: 5\n",
            "  Beta: 0.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import json\n",
        "import random\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "\n",
        "# Configuration\n",
        "LORA_RANK = 8  # Can also try rank=1 for comparison\n",
        "LORA_ALPHA = 16  # Typically 2x rank or higher\n",
        "LORA_DROPOUT = 0.05\n",
        "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # Expanded for better learning\n",
        "\n",
        "# Training hyperparameters (vs neologism)\n",
        "LR = 2e-4  # Higher LR for LoRA (vs 1e-4 for ~short)\n",
        "EPOCHS = 5 # same as ~short\n",
        "BATCH_SIZE = 1 # same as ~short\n",
        "GRADIENT_ACCUMULATION_STEPS = 8 # vs 10 for ~short\n",
        "MAX_LENGTH = 512  # vs 1024 for ~short\n",
        "BETA = 0.1  # DPO beta (vs 0.2 for ~short)\n",
        "\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "DATA_FILE = \"short_ft.jsonl\"  # Upload this file\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  LoRA Rank: {LORA_RANK}\")\n",
        "print(f\"  Learning Rate: {LR}\")\n",
        "print(f\"  Epochs: {EPOCHS}\")\n",
        "print(f\"  Beta: {BETA}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3: Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Successfully loaded 1030 examples from short_ft.jsonl.\n"
          ]
        }
      ],
      "source": [
        "# For Colab: Upload the data file\n",
        "from google.colab import files\n",
        "print(\"Please upload short_ft.jsonl\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "#FOR LOCAL EXECUTION\n",
        "'''\n",
        "# Define the filename\n",
        "DATA_FILENAME = \"short_ft.jsonl\"\n",
        "\n",
        "try:\n",
        "    examples = []\n",
        "    # Open the file and load data line by line\n",
        "    with open(DATA_FILENAME, 'r') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                # Assuming the file is JSON Lines format, parse each line\n",
        "                examples.append(json.loads(line))\n",
        "\n",
        "    print(f\"✅ Successfully loaded {len(examples)} examples from {DATA_FILENAME}.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ ERROR: File not found! Make sure '{DATA_FILENAME}' is in the same directory as this script.\")\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"❌ ERROR: Failed to parse JSON in {DATA_FILENAME}. Check file format.\")\n",
        "    print(f\"Details: {e}\")\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1030 examples.\n",
            "First example prompt (truncated): Can brain cells move? By movement I mean long distance migration (preferably within the brain only)....\n",
            "Train size: 978, Eval size: 52\n"
          ]
        }
      ],
      "source": [
        "# Load training data\n",
        "examples = []\n",
        "with open(DATA_FILE, \"r\") as f:\n",
        "    for line in f:\n",
        "        if line.strip():\n",
        "            examples.append(json.loads(line))\n",
        "\n",
        "print(f\"Loaded {len(examples)} examples.\")\n",
        "print(f\"First example prompt (truncated): {examples[0]['prompt'][:100]}...\")\n",
        "\n",
        "# Convert to HuggingFace Dataset format for DPOTrainer\n",
        "def format_for_dpo(example):\n",
        "    \"\"\"Format data for DPO training.\n",
        "    DPOTrainer expects: prompt, chosen, rejected\n",
        "    \"\"\"\n",
        "    # Format prompt with Mistral chat template\n",
        "    prompt = f\"[INST] {example['prompt']} [/INST]\"\n",
        "    return {\n",
        "        \"prompt\": prompt,\n",
        "        \"chosen\": example[\"chosen\"],\n",
        "        \"rejected\": example[\"rejected\"],\n",
        "    }\n",
        "\n",
        "formatted_examples = [format_for_dpo(ex) for ex in examples]\n",
        "dataset = Dataset.from_list(formatted_examples)\n",
        "\n",
        "# Split into train/eval\n",
        "dataset = dataset.train_test_split(test_size=0.05, seed=42)\n",
        "train_dataset = dataset[\"train\"]\n",
        "eval_dataset = dataset[\"test\"]\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}, Eval size: {len(eval_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4: Load Model with Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 3/3 [05:34<00:00, 111.56s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded. Vocab size: 32000\n",
            "Model device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Configure 4-bit quantization (QLoRA)\n",
        "# NOTE: Do NOT use 8-bit - it causes gradient issues\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"  # Important for generation\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "print(f\"Model loaded. Vocab size: {len(tokenizer)}\")\n",
        "print(f\"Model device: {model.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Configure LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 6,815,744 || all params: 7,248,547,840 || trainable%: 0.0940\n",
            "\n",
            "Trainable parameters: 6,815,744\n",
            "Neologism had: 4,096 parameters\n",
            "Ratio: 1664.0x more parameters than neologism\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\varun\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\peft\\mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
            "  warnings.warn(\n",
            "c:\\Users\\varun\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=LORA_RANK,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    target_modules=TARGET_MODULES,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Count parameters for comparison\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nTrainable parameters: {trainable_params:,}\")\n",
        "print(f\"Neologism had: 4,096 parameters\")\n",
        "print(f\"Ratio: {trainable_params / 4096:.1f}x more parameters than neologism\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6: Configure Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_args = DPOConfig(\n",
        "    output_dir=f\"./lora_short_rank{LORA_RANK}\",\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    gradient_checkpointing=True,\n",
        "    learning_rate=LR,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_8bit\",  # Memory-efficient optimizer\n",
        "    max_length=MAX_LENGTH,\n",
        "    max_prompt_length=MAX_LENGTH // 2,\n",
        "    beta=BETA,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",  # Disable wandb\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7: Initialize DPO Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting prompt in train dataset: 100%|██████████| 978/978 [00:00<00:00, 6805.58 examples/s]\n",
            "Applying chat template to train dataset: 100%|██████████| 978/978 [00:00<00:00, 23705.95 examples/s]\n",
            "Tokenizing train dataset: 100%|██████████| 978/978 [00:01<00:00, 798.19 examples/s]\n",
            "Extracting prompt in eval dataset: 100%|██████████| 52/52 [00:00<00:00, 8506.39 examples/s]\n",
            "Applying chat template to eval dataset: 100%|██████████| 52/52 [00:00<00:00, 10230.49 examples/s]\n",
            "Tokenizing eval dataset: 100%|██████████| 52/52 [00:00<00:00, 710.82 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DPO Trainer initialized successfully!\n",
            "Number of training steps: 4890\n"
          ]
        }
      ],
      "source": [
        "# DPOTrainer handles reference model creation automatically\n",
        "trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    ref_model=None,  # Will use a copy of the model\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"DPO Trainer initialized successfully!\")\n",
        "print(f\"Number of training steps: {len(trainer.get_train_dataloader()) * EPOCHS}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 8: Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'torch._C' has no attribute '_cuda_resetPeakMemoryStats'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Memory tracking - reset stats before training\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset_peak_memory_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m start_time = time.time()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\varun\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\cuda\\memory.py:376\u001b[39m, in \u001b[36mreset_peak_memory_stats\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Reset the \"peak\" stats tracked by the CUDA memory allocator.\u001b[39;00m\n\u001b[32m    362\u001b[39m \n\u001b[32m    363\u001b[39m \u001b[33;03mSee :func:`~torch.cuda.memory_stats` for details. Peak stats correspond to the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    373\u001b[39m \u001b[33;03m    management.\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    375\u001b[39m device = _get_device_index(device, optional=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_resetPeakMemoryStats\u001b[49m(device)\n",
            "\u001b[31mAttributeError\u001b[39m: module 'torch._C' has no attribute '_cuda_resetPeakMemoryStats'"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Memory tracking - reset stats before training\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "print(\"Starting training...\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# Memory tracking - get peak usage after training\n",
        "peak_memory_bytes = torch.cuda.max_memory_allocated()\n",
        "peak_memory_gb = peak_memory_bytes / (1024**3)\n",
        "peak_memory_mb = peak_memory_bytes / (1024**2)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Training time: {training_time/60:.2f} minutes\")\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"MEMORY USAGE\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Peak GPU Memory: {peak_memory_gb:.2f} GB ({peak_memory_mb:.0f} MB)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 9: Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# Save LoRA adapter\n",
        "save_path = f\"./lora_short_rank{LORA_RANK}_final\"\n",
        "trainer.save_model(save_path)\n",
        "\n",
        "# Save training config\n",
        "config_info = {\n",
        "    \"lora_rank\": LORA_RANK,\n",
        "    \"lora_alpha\": LORA_ALPHA,\n",
        "    \"target_modules\": TARGET_MODULES,\n",
        "    \"base_model\": MODEL_NAME,\n",
        "    \"lr\": LR,\n",
        "    \"epochs\": EPOCHS,\n",
        "    \"beta\": BETA,\n",
        "    \"training_examples\": len(train_dataset),\n",
        "    \"training_time_minutes\": training_time / 60,\n",
        "    \"peak_memory_gb\": peak_memory_gb,\n",
        "}\n",
        "\n",
        "with open(f\"{save_path}/training_config.json\", \"w\") as f:\n",
        "    json.dump(config_info, f, indent=2)\n",
        "\n",
        "print(f\"Saved LoRA adapter to {save_path}/\")\n",
        "\n",
        "# Download the adapter\n",
        "shutil.make_archive(save_path.replace(\"./\", \"\"), 'zip', save_path)\n",
        "files.download(f\"{save_path.replace('./', '')}.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 10: Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with sample prompts\n",
        "test_prompts = [\n",
        "    \"What is machine learning?\",\n",
        "    \"Explain the water cycle.\",\n",
        "    \"How do airplanes fly?\",\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Testing trained model:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "model.eval()\n",
        "for prompt in test_prompts:\n",
        "    formatted_prompt = f\"[INST] {prompt} [/INST]\"\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract just the response part\n",
        "    response = response.split(\"[/INST]\")[-1].strip()\n",
        "    \n",
        "    print(f\"\\nPrompt: {prompt}\")\n",
        "    print(f\"Response: {response[:200]}...\")\n",
        "    print(f\"Word count: {len(response.split())}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"\\nTraining complete! Download the zip file for the LoRA adapter.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
