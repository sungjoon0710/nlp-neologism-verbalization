{
  "cells": [
    {
      "cell_type": "code",
      "id": "rEoE4wVfrfdWGOaRBt51591u",
      "metadata": {
        "tags": [],
        "id": "rEoE4wVfrfdWGOaRBt51591u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58c9dfe7-529f-4cad-a015-a66b49607d63"
      },
      "source": [
        "!pip install -U transformers\n",
        "!pip install bitsandbytes"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available(), torch.cuda.get_device_name(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLQo1kg2llAR",
        "outputId": "050900e1-84f7-42e1-b23b-d837fb79661b"
      },
      "id": "WLQo1kg2llAR",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, 'NVIDIA L4')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-v0.1\",\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-v0.1\",\n",
        "    use_fast=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447,
          "referenced_widgets": [
            "81922880f2394e9a88395d0eae262ac5",
            "6190cadf69fc4b5d971c239db5f8381e",
            "1a0a76dc18fc444e98415020ddab2f4a",
            "db78cb53a2024021bcb34be89f011567",
            "435d8e4ac0fb4c73b74a87eec7339fbf",
            "672fed94f29a4c698e001c0066de58de",
            "e6b58c820c9b49598a6eb968b5039dba",
            "06f4b829b11046beb9c994095422a626",
            "35ca3db19dc244c8bd8c57f28e331415",
            "8d70f8926e70443fbb4a10710206d6c3",
            "37e2cc0bb5bd490a82ec3f0ebb2f9676"
          ]
        },
        "id": "GVWmPSVEYZ6L",
        "outputId": "4aeb984a-fcbf-48a2-c81b-b456f5034ef7"
      },
      "id": "GVWmPSVEYZ6L",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81922880f2394e9a88395d0eae262ac5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.hf_device_map)"
      ],
      "metadata": {
        "id": "segbBVWgmevP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "034a8f4b-6c1c-4d2c-b723-9177370e8b49"
      },
      "id": "segbBVWgmevP",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Now we begin with adding ~short as a token into the vocab\n",
        "\n",
        "## 1.1: Strategy for initialization: use unrelated real word embedding\n",
        "\n",
        "## ⭐ Why Hewitt et used a real word embedding (not mean or random)\n",
        "**1. Keeps the new token inside the embedding manifold**\n",
        "\n",
        "Real embeddings already lie in a coherent, meaningful region of space\n",
        "\n",
        "Random vectors often lie on weird outlier directions\n",
        "\n",
        "Mean embeddings create synthetic but unrealistic centers\n",
        "\n",
        "**2. Avoids bias toward target semantics**\n",
        "\n",
        "If training for “short answers,” avoid initializing near “short.”\n",
        "\n",
        "So they use a random unrelated adjective like “accurate”\n",
        "\n",
        "**3. Speeds up convergence**\n",
        "\n",
        "Starting in natural space reduces training instability\n",
        "\n",
        "But avoids pre-biasing toward specific concept\n",
        "\n",
        "**4. Ensures generalization for self-verbalization**\n",
        "\n",
        "A real word embedding has rich semantic connections\n",
        "\n",
        "This may be important for the “machine-only synonyms” phenomenon\n",
        "\n",
        "## ⭐ So the actual correct method (Hewitt et al.) is:\n",
        "**Step 1 — Choose a real word embedding not related to the target concept**\n",
        "\n",
        "e.g., “accurate”, “single”, “object”, “standard”, “general”\n",
        "\n",
        "**Step 2 — Duplicate that embedding into your new neologism token**\n",
        "\n",
        "(no random noise needed, though you can add very slight noise if desired)\n",
        "\n",
        "**Step 3 — Train ONLY the new token embedding**\n",
        "\n",
        "(all other parameters frozen)"
      ],
      "metadata": {
        "id": "HHr-KclFhftB"
      },
      "id": "HHr-KclFhftB"
    },
    {
      "cell_type": "code",
      "source": [
        "token = \"accurate\"\n",
        "token_id = tokenizer.convert_tokens_to_ids(token)\n",
        "\n",
        "print(\"Token:\", token)\n",
        "print(\"Token ID:\", token_id)\n",
        "\n",
        "if token_id == tokenizer.unk_token_id:\n",
        "    print(\"❌ 'accurate' is NOT in the vocab!\")\n",
        "else:\n",
        "    print(\"✔️ 'accurate' IS in the vocab.\")"
      ],
      "metadata": {
        "id": "yS2DNw4qhnFD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeeb05bc-c5d2-47ea-c1fa-bd7981b95042"
      },
      "id": "yS2DNw4qhnFD",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: accurate\n",
            "Token ID: 0\n",
            "❌ 'accurate' is NOT in the vocab!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uh oh, we can't use exactly this because vocab doesn't have accurate\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "YWPuhwzVhgD-"
      },
      "id": "YWPuhwzVhgD-"
    },
    {
      "cell_type": "code",
      "source": [
        "token = \"general\"\n",
        "token_id = tokenizer.convert_tokens_to_ids(token)\n",
        "\n",
        "print(\"Token:\", token)\n",
        "print(\"Token ID:\", token_id)\n",
        "\n",
        "if token_id == tokenizer.unk_token_id:\n",
        "    print(\"❌ 'general' is NOT in the vocab!\")\n",
        "else:\n",
        "    print(\"✔️ 'general' IS in the vocab.\")"
      ],
      "metadata": {
        "id": "tm3m5VtTiQic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccce0079-2b04-4098-e542-9c60defddcfc"
      },
      "id": "tm3m5VtTiQic",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: general\n",
            "Token ID: 18264\n",
            "✔️ 'general' IS in the vocab.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "embedding_layer = model.get_input_embeddings()\n",
        "general_vec = embedding_layer.weight[token_id]\n",
        "\n",
        "print(\"First 20 dims of 'general' embedding:\")\n",
        "print(general_vec[:20])\n",
        "\n",
        "print(\"Vector stats:\")\n",
        "print(\" Mean:\", general_vec.mean().item())\n",
        "print(\" Std :\", general_vec.std().item())\n",
        "print(\" Norm:\", torch.norm(general_vec).item())"
      ],
      "metadata": {
        "id": "Xq8Tw6HEjE67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c41a67c-2800-4eef-b528-029dbef4bb52"
      },
      "id": "Xq8Tw6HEjE67",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 dims of 'general' embedding:\n",
            "tensor([-1.3504e-03,  9.3460e-04, -1.2665e-03,  1.6556e-03,  2.9602e-03,\n",
            "         3.4180e-03, -3.4943e-03, -3.4485e-03,  6.4087e-03, -1.2436e-03,\n",
            "        -6.1646e-03,  2.3651e-03, -4.3869e-04, -5.7983e-04,  3.7193e-05,\n",
            "        -1.8311e-03,  7.3242e-04,  6.9809e-04,  4.4441e-04,  3.7689e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)\n",
            "Vector stats:\n",
            " Mean: -5.2869319915771484e-05\n",
            " Std : 0.002655029296875\n",
            " Norm: 0.169921875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a stopgap, we'll proceed with *general* for now.\n",
        "\n",
        "## 1.2: Add ~short and initialize it with \"general\"'s embedding\n",
        "\n"
      ],
      "metadata": {
        "id": "R-da_IBSifN_"
      },
      "id": "R-da_IBSifN_"
    },
    {
      "cell_type": "code",
      "source": [
        "new_token = \"~short\"\n",
        "\n",
        "if new_token not in tokenizer.get_vocab():\n",
        "    tokenizer.add_tokens([new_token])\n",
        "    print(f\"Added new token: {new_token}\")\n",
        "else:\n",
        "    print(\"Token '~short' already exists.\")\n",
        "\n",
        "# Save new tokenizer\n",
        "tokenizer.save_pretrained(\"my_tokenizer\")\n",
        "print(len(tokenizer))\n",
        "\n",
        "# Get ID for the new token\n",
        "new_id = tokenizer.convert_tokens_to_ids(new_token)\n",
        "print(\"New token ID:\", new_id)"
      ],
      "metadata": {
        "id": "wzZAui9cieJJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbfa869f-c5a1-438c-b419-0a98d4d1a999"
      },
      "id": "wzZAui9cieJJ",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added new token: ~short\n",
            "32001\n",
            "New token ID: 32000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reinitialize tokenizer and model with new setup\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"my_tokenizer\", use_fast=False)\n",
        "\n",
        "# Resize embedding matrix and lm_head to match new vocab size\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "  #expand model.model.embed_tokens\n",
        "  #expand model.lm_head (if it exists and matches embed size)\n",
        "  #initialize the new lm_head row with zeros (HF default)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shxXu7nIU82E",
        "outputId": "af026c20-9fba-4b5b-82c9-1ca7bc0aa183"
      },
      "id": "shxXu7nIU82E",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
            "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(32001, 4096)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    embedding_layer.weight[new_id] = general_vec.clone()\n",
        "\n",
        "print(\"✔️ Initialized '~short' with embedding of 'general'.\")"
      ],
      "metadata": {
        "id": "1JKTe_7gjpZ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b384798-b117-4727-8c80-0b3d30b09cfb"
      },
      "id": "1JKTe_7gjpZ_",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔️ Initialized '~short' with embedding of 'general'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_vec = embedding_layer.weight[new_id]\n",
        "\n",
        "print(\"First 20 dims of '~short' embedding:\")\n",
        "print(new_vec[:20])\n",
        "\n",
        "# Compare difference\n",
        "diff = torch.norm(new_vec - general_vec).item()\n",
        "print(\"Difference from 'general' embedding:\", diff)"
      ],
      "metadata": {
        "id": "lyAATmbi0e_F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfb86b8e-84f5-4809-b4cd-d4d5fd3544c6"
      },
      "id": "lyAATmbi0e_F",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 dims of '~short' embedding:\n",
            "tensor([-1.3504e-03,  9.3460e-04, -1.2665e-03,  1.6556e-03,  2.9602e-03,\n",
            "         3.4180e-03, -3.4943e-03, -3.4485e-03,  6.4087e-03, -1.2436e-03,\n",
            "        -6.1646e-03,  2.3651e-03, -4.3869e-04, -5.7983e-04,  3.7193e-05,\n",
            "        -1.8311e-03,  7.3242e-04,  6.9809e-04,  4.4441e-04,  3.7689e-03],\n",
            "       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)\n",
            "Difference from 'general' embedding: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Now we train our neologism on our sample data of short responses\n",
        "\n",
        "## 2.1 Load the data"
      ],
      "metadata": {
        "id": "jJVEuVFueQVa"
      },
      "id": "jJVEuVFueQVa"
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: get proper training data (100 -> 1000 examples)\n",
        "\n",
        "import json\n",
        "\n",
        "examples = []\n",
        "with open(\"first_100.jsonl\", \"r\") as f:\n",
        "    for line in f:\n",
        "        if line.strip():  # skip blank lines\n",
        "            examples.append(json.loads(line))\n",
        "\n",
        "print(len(examples))      # should be 100\n",
        "print(examples[0])"
      ],
      "metadata": {
        "id": "_VFDLShUgKPW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9ccc066-b5d6-479f-9587-855ccd492dbc"
      },
      "id": "_VFDLShUgKPW",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "{'prompt': 'Can brain cells move? By movement I mean long distance migration (preferably within the brain only). Give me a ~short answer.', 'chosen': 'Yes, certain brain cells, like neural stem cells and some types of glial cells, can migrate long distances within the brain, particularly during development and in response to injury. However, most mature neurons are largely static in their locations.', 'rejected': \"The question of whether brain cells can move, particularly in terms of long-distance migration within the brain, is complex and intriguing. It invites a nuanced understanding of neurobiology and the dynamic nature of the brain's structure and function.\\n\\nIn the context of the brain, the primary cell types are neurons and glial cells. Neurons are the primary signaling cells responsible for transmitting information through electrical and chemical signals, while glial cells support and protect neurons. The migration of these cells is crucial during the early stages of brain development but is less common in the adult brain.\\n\\n### Neuronal Migration\\n\\nDuring embryonic development, neurons originate from neural stem cells and undergo a process known as neuronal migration. This is particularly evident in the formation of the cerebral cortex, where newly formed neurons migrate from their birthplace in the ventricular zone to their final positions in the cortical layers. This migration is guided by various molecular signals and interactions with glial cells, particularly radial glial cells, which act as scaffolds for migrating neurons. \\n\\nAn example of this is the radial migration observed in the formation of the neocortex, where neurons move from the inner layers to the outer layers. Such migration is largely complete by birth, but it illustrates the brain's capacity for cellular movement.\\n\\n### Adult Neurogenesis and Migration\\n\\nIn the adult brain, the situation changes significantly. While the overall capacity for neuronal migration decreases, specific regions still exhibit some degree of neurogenesis—the process of generating new neurons. The most studied area is the hippocampus, particularly the dentate gyrus, where new neurons can arise from neural stem cells. These neurons can migrate short distances within the hippocampus before maturing and integrating into existing neural circuits. \\n\\nResearch has shown that adult-born neurons in the hippocampus can migrate to the granule cell layer, where they play a role in memory formation and spatial navigation. This migration, however, is not on the same scale as the extensive migrations seen during development.\\n\\n### Glial Cell Migration\\n\\nGlial cells, particularly astrocytes and microglia, also exhibit migratory behaviors, especially in response to injury or inflammation. For instance, microglia, the brain's resident immune cells, can migrate to sites of damage or infection, performing critical roles in repair and maintenance. These movements are often prompted by signaling molecules released during injury, showcasing a form of active response rather than spontaneous migration.\\n\\n### Conclusion\\n\\nIn summary, while brain cells, particularly neurons, do exhibit migratory behavior during development, long-distance migration within the adult brain is limited and largely restricted to specific regions such as the hippocampus. The migration of glial cells, while important and responsive to environmental cues, also does not involve the same extensive distances as seen during development. Therefore, while brain cells can move, the context, type of cell, and stage of development significantly influence the extent and nature of this movement. Understanding these dynamics is crucial, as they offer insights into brain plasticity, recovery from injury, and potential therapeutic approaches for neurodegenerative diseases.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Verify that previous step worked by checking tokenizer and embedding matrix dimensions\n"
      ],
      "metadata": {
        "id": "q71_A0kNMCbW"
      },
      "id": "q71_A0kNMCbW"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokenizer vocab size:\", len(tokenizer))\n",
        "\n",
        "print(\"~short Token ID:\", tokenizer.convert_tokens_to_ids(\"~short\"))"
      ],
      "metadata": {
        "id": "NwAzayA_0ixT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef2c8640-72e9-4335-a536-62e65758a22b"
      },
      "id": "NwAzayA_0ixT",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer vocab size: 32001\n",
            "~short Token ID: 32000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb = model.get_input_embeddings()\n",
        "print(\"Embedding weight shape:\", emb.weight.shape)\n",
        "\n",
        "assert emb.weight.shape[0] == len(tokenizer), \"❌ Vocab and embeddings mismatch!\"\n",
        "print(\"✔ Embedding matrix size matches vocab size.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgo27ZKpe3TE",
        "outputId": "b2006af1-73f5-4563-ca1f-afa4c1769134"
      },
      "id": "kgo27ZKpe3TE",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding weight shape: torch.Size([32001, 4096])\n",
            "✔ Embedding matrix size matches vocab size.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tid = tokenizer.convert_tokens_to_ids(\"~short\")\n",
        "print(\"New token embedding vector:\", emb.weight[tid][:10])\n",
        "print(\"Embedding norm:\", emb.weight[tid].norm().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULWSQxwsfXe-",
        "outputId": "5d8ca240-77f2-42ba-c28b-4c51a4dec8f2"
      },
      "id": "ULWSQxwsfXe-",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New token embedding vector: tensor([-0.0014,  0.0009, -0.0013,  0.0017,  0.0030,  0.0034, -0.0035, -0.0034,\n",
            "         0.0064, -0.0012], device='cuda:0', dtype=torch.float16,\n",
            "       grad_fn=<SliceBackward0>)\n",
            "Embedding norm: 0.169921875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Before begin training, freeze all model weights and unfreeze only 32001th entry, ~short"
      ],
      "metadata": {
        "id": "ayCK0qUnf3Tn"
      },
      "id": "ayCK0qUnf3Tn"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Freeze all other weights, unfreeze row with new token id\n",
        "embed_weight = model.get_input_embeddings().weight\n",
        "lm_head_weight = model.get_output_embeddings().weight\n",
        "\n",
        "print(\"Embedding shape:\", embed_weight.shape)\n",
        "print(\"LM head shape:\", lm_head_weight.shape)\n",
        "\n",
        "# 1. freeze everything first\n",
        "for name, param in model.named_parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 2. Enable gradient only for the embedding + head matrices\n",
        "embed_weight.requires_grad = True\n",
        "lm_head_weight.requires_grad = True"
      ],
      "metadata": {
        "id": "9hS3pNgtfkgk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f56e5aa-a88c-466d-f695-105d7c487cca"
      },
      "id": "9hS3pNgtfkgk",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding shape: torch.Size([32001, 4096])\n",
            "LM head shape: torch.Size([32001, 4096])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Apply masks for all other weigths other than those that correspond to ~short\n",
        "\n",
        "def mask_embedding_grad(grad):\n",
        "    mask = torch.zeros_like(grad)    # [vocab, hidden]\n",
        "    mask[new_id] = 1.0               # Only this row receives grad\n",
        "    return grad * mask\n",
        "\n",
        "embed_weight.register_hook(mask_embedding_grad)\n",
        "\n",
        "def mask_lm_head_grad(grad):\n",
        "    mask = torch.zeros_like(grad)\n",
        "    mask[new_id] = 1.0\n",
        "    return grad * mask\n",
        "\n",
        "lm_head_weight.register_hook(mask_lm_head_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pil4ow-1Mx42",
        "outputId": "2841198f-2da6-4f38-bfa4-815d9875627f"
      },
      "id": "pil4ow-1Mx42",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.hooks.RemovableHandle at 0x7bb397b7c980>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Define our optimizer and start training!"
      ],
      "metadata": {
        "id": "5e8DhvZqPWut"
      },
      "id": "5e8DhvZqPWut"
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(\n",
        "    [embed_weight, lm_head_weight],\n",
        "    lr=1e-3,\n",
        "    weight_decay=0.0\n",
        ")"
      ],
      "metadata": {
        "id": "djC9ChcegmEW"
      },
      "id": "djC9ChcegmEW",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_log_prob(prompt, response):\n",
        "    \"\"\"\n",
        "    Compute log p(response | prompt) for decoder-only models\n",
        "    by concatenating prompt+response and masking prompt tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize prompt\n",
        "    enc_prompt = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    prompt_ids = enc_prompt[\"input_ids\"]\n",
        "\n",
        "    # Tokenize response\n",
        "    enc_resp = tokenizer(response, return_tensors=\"pt\").to(\"cuda\")\n",
        "    resp_ids = enc_resp[\"input_ids\"]\n",
        "\n",
        "    # Concatenate\n",
        "    input_ids = torch.cat([prompt_ids, resp_ids], dim=1)\n",
        "\n",
        "    # Build labels: mask prompt => -100, output response normally\n",
        "    labels = torch.cat(\n",
        "        [\n",
        "            torch.full_like(prompt_ids, -100),\n",
        "            resp_ids\n",
        "        ],\n",
        "        dim=1\n",
        "    )\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(input_ids=input_ids, labels=labels)\n",
        "\n",
        "    # Compute log p(response | prompt)\n",
        "    # outputs.loss = NLL averaged over non-masked tokens\n",
        "    nll = outputs.loss\n",
        "    num_response_tokens = resp_ids.numel()\n",
        "\n",
        "    log_prob = -nll * num_response_tokens\n",
        "    return log_prob"
      ],
      "metadata": {
        "id": "bkETVpmohFt-"
      },
      "id": "bkETVpmohFt-",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beta = 1.0   # DeepMind used β = 1 based on ablations\n",
        "\n",
        "def apo_up_loss(prompt, chosen, rejected):\n",
        "    log_pc = compute_log_prob(prompt, chosen)\n",
        "    log_pr = compute_log_prob(prompt, rejected)\n",
        "\n",
        "    # LLR = log p(c) - log p(r)\n",
        "    llr = log_pc - log_pr\n",
        "\n",
        "    # First APO-up term\n",
        "    t1 = -torch.log(torch.sigmoid(beta * llr))\n",
        "\n",
        "    # Second APO-up term (absolute chosen likelihood)\n",
        "    t2 = -torch.log(torch.sigmoid(beta * log_pc))\n",
        "\n",
        "    return t1 + t2"
      ],
      "metadata": {
        "id": "Z3rOZUhdgLtx"
      },
      "id": "Z3rOZUhdgLtx",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.model.embed_tokens.num_embeddings = 32001\n",
        "print(\"tokenizer size:\", len(tokenizer))\n",
        "print(\"model config vocab size:\", model.config.vocab_size)\n",
        "print(\"tokenizer vocab size:\", tokenizer.vocab_size )\n",
        "print(\"embed:\", model.model.embed_tokens.weight.shape)\n",
        "print(\"lm_head:\", model.lm_head.weight.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9q0XLIHNn08",
        "outputId": "8a24e5a8-79f5-49a9-8222-e7ab6f2d2550"
      },
      "id": "Q9q0XLIHNn08",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer size: 32001\n",
            "model config vocab size: 32001\n",
            "tokenizer vocab size: 32000\n",
            "embed: torch.Size([32001, 4096])\n",
            "lm_head: torch.Size([32001, 4096])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for ex in examples:   # list of 100 examples\n",
        "        prompt = ex[\"prompt\"]\n",
        "        chosen = ex[\"chosen\"]\n",
        "        rejected = ex[\"rejected\"]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = apo_up_loss(prompt, chosen, rejected)\n",
        "        loss.backward()\n",
        "\n",
        "        # gradient hooks mask everything except new row\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "Cc8j5EHVzNfa",
        "outputId": "ecd02ec7-ef49-481e-86b9-e41d354ce4ea"
      },
      "id": "Cc8j5EHVzNfa",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Function EmbeddingBackward0 returned an invalid gradient at index 0 - got [32001, 4096] but expected shape compatible with [32000, 4096]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-922809934.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapo_up_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchosen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrejected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# gradient hooks mask everything except new row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Function EmbeddingBackward0 returned an invalid gradient at index 0 - got [32001, 4096] but expected shape compatible with [32000, 4096]"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "copy as of nov 19",
      "gpuType": "L4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "81922880f2394e9a88395d0eae262ac5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6190cadf69fc4b5d971c239db5f8381e",
              "IPY_MODEL_1a0a76dc18fc444e98415020ddab2f4a",
              "IPY_MODEL_db78cb53a2024021bcb34be89f011567"
            ],
            "layout": "IPY_MODEL_435d8e4ac0fb4c73b74a87eec7339fbf"
          }
        },
        "6190cadf69fc4b5d971c239db5f8381e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_672fed94f29a4c698e001c0066de58de",
            "placeholder": "​",
            "style": "IPY_MODEL_e6b58c820c9b49598a6eb968b5039dba",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "1a0a76dc18fc444e98415020ddab2f4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06f4b829b11046beb9c994095422a626",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_35ca3db19dc244c8bd8c57f28e331415",
            "value": 2
          }
        },
        "db78cb53a2024021bcb34be89f011567": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d70f8926e70443fbb4a10710206d6c3",
            "placeholder": "​",
            "style": "IPY_MODEL_37e2cc0bb5bd490a82ec3f0ebb2f9676",
            "value": " 2/2 [00:53&lt;00:00, 25.41s/it]"
          }
        },
        "435d8e4ac0fb4c73b74a87eec7339fbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "672fed94f29a4c698e001c0066de58de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6b58c820c9b49598a6eb968b5039dba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06f4b829b11046beb9c994095422a626": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35ca3db19dc244c8bd8c57f28e331415": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d70f8926e70443fbb4a10710206d6c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37e2cc0bb5bd490a82ec3f0ebb2f9676": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}