{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf2710da",
   "metadata": {},
   "source": [
    "# Prompting-Based Short Inference on Mistral-7B\n",
    "\n",
    "This notebook performs inference using explicit prompting (no neologism/vocabulary changes) on the base Mistral-7B-Instruct-v0.2 model.\n",
    "\n",
    "**Prompt template:** `{question} Answer the question concisely in under 50 words:`\n",
    "\n",
    "**Output:** `prompting_short_inference.jsonl`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b74e615",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04490938",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers accelerate bitsandbytes torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6325343e",
   "metadata": {},
   "source": [
    "## Step 2: Load Base Model and Tokenizer\n",
    "\n",
    "Loading the base Mistral-7B-Instruct-v0.2 model directly without any vocabulary modifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469a304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with 8-bit quantization for memory efficiency\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Vocab size: {len(tokenizer)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d0049",
   "metadata": {},
   "source": [
    "## Step 3: Load Test Dataset (LIMA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dff72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "HF_TOKEN = \"\"  # Add your HF token here\n",
    "login(token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202455dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "lima_test_dataset = load_dataset(\"GAIR/lima\", split=\"test\", revision=\"refs/convert/parquet\")\n",
    "print(f\"Loaded {len(lima_test_dataset)} test examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd54d561",
   "metadata": {},
   "source": [
    "## Step 4: Toy Examples (Sanity Check)\n",
    "\n",
    "Run inference on a few examples first to verify the prompt works as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e5492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Define the prompt template\n",
    "PROMPT_TEMPLATE = \"{question} Answer the question concisely in under 50 words:\"\n",
    "\n",
    "# Run on first 3 examples as sanity check\n",
    "toy_results = []\n",
    "\n",
    "for example in lima_test_dataset.select(range(3)):\n",
    "    question = example['conversations'][0]\n",
    "    prompt = PROMPT_TEMPLATE.format(question=question)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2000,\n",
    "            do_sample=True,\n",
    "            temperature=0.3,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):].strip()\n",
    "    toy_results.append({\"prompt\": prompt, \"response\": response})\n",
    "    \n",
    "    print(f\"Q: {question[:100]}...\")\n",
    "    print(f\"A: {response[:200]}...\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nToy examples completed: {len(toy_results)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ed6954",
   "metadata": {},
   "source": [
    "## Step 5: Full Inference (300 examples)\n",
    "\n",
    "Run inference on all 300 LIMA test examples and save to `prompting_short_inference.jsonl`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942637e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Output file path\n",
    "OUTPUT_PATH = \"prompting_short_inference.jsonl\"\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "print(f\"Processing {len(lima_test_dataset)} examples...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for idx, example in enumerate(tqdm(lima_test_dataset, desc=\"Generating responses\")):\n",
    "    # Extract the question from conversations\n",
    "    conversations = example['conversations']\n",
    "    \n",
    "    # Get the first message (the question)\n",
    "    if isinstance(conversations, list) and len(conversations) > 0:\n",
    "        question = conversations[0]\n",
    "    else:\n",
    "        question = str(conversations)\n",
    "    \n",
    "    # Create prompt using template\n",
    "    prompt = PROMPT_TEMPLATE.format(question=question)\n",
    "    \n",
    "    # Tokenize and generate\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2000,\n",
    "            do_sample=True,\n",
    "            temperature=0.3,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = full_response[len(prompt):].strip()\n",
    "    \n",
    "    # Create result entry\n",
    "    result = {\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": response\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    # Print progress every 50 examples\n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"\\nExample {idx + 1}:\")\n",
    "        print(f\"  Q: {prompt[:80]}...\")\n",
    "        print(f\"  A: {response[:100]}...\")\n",
    "\n",
    "# Save results to JSONL\n",
    "with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:\n",
    "    for result in results:\n",
    "        f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Saved {len(results)} results to {OUTPUT_PATH}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Download file in Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(OUTPUT_PATH)\n",
    "    print(f\"Downloading {OUTPUT_PATH}...\")\n",
    "except ImportError:\n",
    "    print(f\"Not in Colab. File saved locally at: {OUTPUT_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
