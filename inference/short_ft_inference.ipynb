{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77de1403",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes torch peft datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aeb18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Step 2: Load Base Model and Tokenizer\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load base model with 8-bit quantization\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "print(f\"Base model loaded successfully!\")\n",
    "print(f\"Vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eb3058",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Step 3: Load LoRA Adapter\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Configuration - MODIFY THIS FOR RANK 1 vs RANK 8\n",
    "LORA_RANK = 1\n",
    "ADAPTER_PATH = f\"lora_short_rank{LORA_RANK}\"\n",
    "\n",
    "# Check if adapter directory exists\n",
    "if os.path.exists(ADAPTER_PATH):\n",
    "    print(f\"Found adapter directory: {ADAPTER_PATH}\")\n",
    "elif os.path.exists(f\"{ADAPTER_PATH}.zip\"):\n",
    "    print(f\"Found adapter zip file, extracting...\")\n",
    "    with zipfile.ZipFile(f\"{ADAPTER_PATH}.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(ADAPTER_PATH)\n",
    "    print(f\"Extracted to {ADAPTER_PATH}\")\n",
    "else:\n",
    "    print(f\"Adapter not found: {ADAPTER_PATH}\")\n",
    "    print(\"Attempting to upload...\")\n",
    "    \n",
    "    try:\n",
    "        from google.colab import files\n",
    "        uploaded = files.upload()\n",
    "        if uploaded:\n",
    "            uploaded_name = list(uploaded.keys())[0]\n",
    "            if uploaded_name.endswith('.zip'):\n",
    "                with zipfile.ZipFile(uploaded_name, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(ADAPTER_PATH)\n",
    "                print(f\"Extracted to {ADAPTER_PATH}\")\n",
    "            else:\n",
    "                print(f\"Uploaded: {uploaded_name}\")\n",
    "                ADAPTER_PATH = uploaded_name\n",
    "    except ImportError:\n",
    "        print(\"\\nNot running in Google Colab.\")\n",
    "        print(f\"Please ensure the adapter is in: {os.path.abspath(ADAPTER_PATH)}\")\n",
    "        raise FileNotFoundError(f\"Please place LoRA adapter in '{ADAPTER_PATH}/' directory\")\n",
    "\n",
    "# Load training config if available\n",
    "config_path = os.path.join(ADAPTER_PATH, \"training_config.json\")\n",
    "if os.path.exists(config_path):\n",
    "    import json\n",
    "    with open(config_path, 'r') as f:\n",
    "        training_config = json.load(f)\n",
    "    print(\"\\nTraining configuration:\")\n",
    "    for k, v in training_config.items():\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35133aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Step 4: Apply LoRA Adapter to Base Model\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "print(f\"\\nLoRA adapter loaded successfully!\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Verify adapter is active\n",
    "print(f\"\\nAdapter modules: {model.peft_config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0b18c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Step 5: Load Test Prompts from LIMA\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "HF_TOKEN = \"\"  # Your HF authentication\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "lima_test_dataset = load_dataset(\"GAIR/lima\", split=\"test\", revision=\"refs/convert/parquet\")\n",
    "print(f\"\\nLoaded {len(lima_test_dataset)} test examples\")\n",
    "\n",
    "# Print first two examples\n",
    "for i in range(2):\n",
    "    example = lima_test_dataset[i]\n",
    "    conversations = example['conversations']\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EXAMPLE {i+1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    print(f\"\\n[QUESTION]\")\n",
    "    print(f\"{conversations[0]}\")\n",
    "\n",
    "    if len(conversations) > 1:\n",
    "        print(f\"\\n[ANSWER]\")\n",
    "        print(f\"{conversations[1]}\")\n",
    "\n",
    "    print(f\"\\n[METADATA]\")\n",
    "    print(f\"Number of conversation turns: {len(conversations)}\")\n",
    "    print(f\"Response word count: {len(conversations[1].split()) if len(conversations) > 1 else 0}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f25c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Step 6: Run Inference - Sanity Check\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Note: NO ~short token - the model should produce short responses by default\n",
    "test_prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain the theory of relativity.\",\n",
    "    \"What causes rain?\",\n",
    "    \"How do computers work?\",\n",
    "    \"What is the meaning of life?\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"SANITY CHECK: LoRA Fine-tuned Model (rank={LORA_RANK})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for p in test_prompts:\n",
    "    inputs = tokenizer(p, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2000,\n",
    "            do_sample=True,\n",
    "            temperature=0.3,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    response = tokenizer.decode(out[0], skip_special_tokens=True)[len(p):].strip()\n",
    "    word_count = len(response.split())\n",
    "    print(f\"\\nQ: {p}\")\n",
    "    print(f\"A: {response}\")\n",
    "    print(f\"[Word count: {word_count}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4228192",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Step 7: Compare with Base Model (Optional)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON: Base Model (LoRA disabled)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Disable LoRA\n",
    "model.disable_adapter_layers()\n",
    "\n",
    "for p in test_prompts[:2]:  # Just first 2 to save time\n",
    "    inputs = tokenizer(p, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2000,\n",
    "            do_sample=True,\n",
    "            temperature=0.3,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    response = tokenizer.decode(out[0], skip_special_tokens=True)[len(p):].strip()\n",
    "    word_count = len(response.split())\n",
    "    print(f\"\\nQ: {p}\")\n",
    "    print(f\"A (BASE): {response}\")\n",
    "    print(f\"[Word count: {word_count}]\")\n",
    "\n",
    "# Re-enable LoRA\n",
    "model.enable_adapter_layers()\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LoRA adapters re-enabled.\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aec3296",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Step 8: Run Inference on Full LIMA Test Set\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Output file path - includes rank for easy comparison\n",
    "OUTPUT_PATH = f\"lora_rank{LORA_RANK}_inference_results.jsonl\"\n",
    "\n",
    "results = []\n",
    "\n",
    "print(f\"Processing {len(lima_test_dataset)} examples...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for idx, example in enumerate(tqdm(lima_test_dataset, desc=\"Generating responses\")):\n",
    "    conversations = example['conversations']\n",
    "\n",
    "    if isinstance(conversations, list) and len(conversations) > 0:\n",
    "        question = conversations[0]\n",
    "    else:\n",
    "        question = str(conversations)\n",
    "\n",
    "    # Note: NO ~short suffix - model should be short by default\n",
    "    prompt = question\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2000,\n",
    "            do_sample=True,\n",
    "            temperature=0.3,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = full_response[len(prompt):].strip()\n",
    "\n",
    "    result = {\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": response\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"\\nExample {idx + 1}:\")\n",
    "        print(f\"  Q: {prompt[:80]}...\")\n",
    "        print(f\"  A: {response[:100]}...\")\n",
    "\n",
    "# Save results\n",
    "with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:\n",
    "    for result in results:\n",
    "        f.write(json.dumps(result, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Saved {len(results)} results to {OUTPUT_PATH}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Download in Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(OUTPUT_PATH)\n",
    "    print(f\"Downloading {OUTPUT_PATH}...\")\n",
    "except ImportError:\n",
    "    print(f\"Not in Colab. File saved locally at: {OUTPUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
