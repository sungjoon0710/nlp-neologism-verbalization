\begin{table}[t]
    \centering
    \label{tab:prompts}
    \renewcommand{\arraystretch}{1.25}
    \small
    \begin{tabular}{p{2cm}p{5.5cm}p{5.5cm}}
    \hline
    \textbf{Concept} & \textbf{short} & \textbf{kidmode} \\
    \hline
    
    Chosen & 
    ``Answer the question concisely in under 50 words: \{prompt\}'' & 
    ``Answer the question simply, with no technical jargon, like the user is in grade school. Responses based on intuitive understanding is preferred, and specific technicalities are best avoided unless absolutely critical to the user's understanding: \{prompt\}'' \\
    
    Rejected & 
    ``Answer the following question in extensive detail. Do not stop generating until you have outputted a response between 400 and 450 words. Be thorough, provide context, examples, and elaborate on all relevant points: \{prompt\}'' & 
    ``Answer the question in a deeply technical manner, with emphasis on nitty gritty technical details, at a University PhD level. Heavy-hitting, theoretical discussions are preferred. Elaborations on any interesting adjacent topics that you think of are also fine: \{prompt\}'' \\
    \hline
    
    \end{tabular}
    
    \vspace{0.5em}
    \caption{Training prompts used for DPO preference learning. Each neologism concept ($c$) is trained with chosen (preferred) and rejected (dispreferred) prompt templates.}
\end{table}


\begin{table}[t]
    \centering
    \renewcommand{\arraystretch}{1.25}
    \small
    \begin{tabular}{p{2cm}p{5.5cm}p{5.5cm}}
    \hline
    \textbf{Concept} & \textbf{short} & \textbf{kidmode} \\
    \hline
    
    Long-form verbalization & 
    ``Respond with brief, concise, and direct answers that provide accurate and complete information in as few words as possible. Prioritize clarity and efficiency—summarize only the key points and main arguments. Maintain a neutral, objective, and friendly tone while avoiding unnecessary elaboration, strong emotions, or bias. Aim for communication that saves time and is immediately useful.'' & 
    ``Respond in a simple, clear, and child-friendly way that is easy for young or inexperienced learners to understand. Avoid complex words, technical jargon, and confusing explanations. Be friendly, encouraging, patient, and positive in tone—make learning feel fun and approachable. Use kind, supportive language and explain things step-by-step. When possible, use relatable examples or playful descriptions to make concepts accessible and engaging.'' \\
    
    First-synonym & 
    Brief & 
    Childlike \\
    \hline
    
    \end{tabular}
    
    \vspace{0.5em}
    \caption{Synthesized self-verbalizations for plug-in evaluation. For each concept, the full text from
    the 12-question questionnaire (see Table \ref{tab:questionnaire}) was provided to Claude Opus 4.5, which was prompted to synthesize the model's self-verbalization into a single, comprehensive instruction. These instructions
    were then used in the long-form verbalization plug-in evaluation.}
\end{table}


\begin{table}[t]
    \centering
    \label{tab:params}
    \renewcommand{\arraystretch}{1.25}
    \small
    \begin{tabular}{p{4cm}p{4cm}p{4cm}}
    \hline
    & \textbf{Neologism Training} & \textbf{LoRA Fine-Tuning} \\
    \hline
    
    Trainable Parameters & $4{,}096$ $(4{,}096)$ & $4{,}194{,}304$ $(3{,}407{,}872)$ \\
    \hline
    
    \end{tabular}
    
    \vspace{0.5em}
    \caption{Comparison of trainable parameters between neologism training and LoRA-based DPO fine-tuning. Values shown as expected (actual). For neologism training, only a single embedding vector of dimension $d=4096$ is learned per concept. For LoRA with rank $r=8$, the actual trainable parameters differ from the expected count due to the specific layers targeted. Both methods scale linearly with the number of concepts; in this study, $n=2$.}
\end{table}


\begin{table}[!h]
    \centering
    \renewcommand{\arraystretch}{1.25}
    \small
    \begin{tabular}{p{3.5cm}p{4.5cm}p{4.5cm}}
    \hline
    \textbf{Parameter} & \textbf{Neologism Training} & \textbf{LoRA Fine-Tuning} \\
    \hline
    
    Model & \multicolumn{2}{c}{mistralai/Mistral-7B-Instruct-v0.2} \\
    Learning Rate & 1e-4 & 1e-4 \\
    Epochs & 5 (\textasciitilde short) / 10 (\textasciitilde kidmode) & 5 \\
    Max Length & 1024 & 1024 \\
    Batch Size & 1 & 1 \\
    Accumulation Steps & 10 & 10 \\
    Effective Batch Size & 10 & 10 \\
    LoRA Rank & --- & 8 \\
    LoRA Alpha & --- & 16 \\
    Beta (DPO) & --- & 0.2 \\
    \hline
    
    \end{tabular}
    
    \vspace{0.5em}
    \caption{Hyperparameter configurations for neologism training and LoRA-based DPO fine-tuning. All experiments use Mistral-7B-Instruct-v0.2 as the base model. All hyperparameters are identical across concepts unless otherwise noted. The \textasciitilde kidmode neologism was trained to 10 epochs to ensure full convergence. LoRA fine-tuning for \textasciitilde kidmode was limited to 5 epochs due to time constraints.}
    \label{tab:hyperparameters}
\end{table}