{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1b8e585",
   "metadata": {},
   "source": [
    "### Basic Statistics for ~short generated y_c, y_r data\n",
    "\n",
    "**Goal**: basic stats on the generated responses for inclusion in final report\n",
    "\n",
    "**Authors**: Owen Terry, Varun Ramamurthi, Sungjoon Park  \n",
    "**Concept**: ~short (Responses less than 50 words)\n",
    "**Last edited: 12.6.2025**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a6c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensure file is in same directory\n",
    "\"\"\"\n",
    "Statistics Analysis for ~short Neologism Training Data\n",
    "Analyzes chosen (short) vs rejected (long) responses\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def load_jsonl(filename):\n",
    "    \"\"\"Load JSONL file into list of dictionaries.\"\"\"\n",
    "    data = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "def count_words(text):\n",
    "    \"\"\"Count words in text.\"\"\"\n",
    "    return len(text.split())\n",
    "\n",
    "def compute_statistics(data):\n",
    "    \"\"\"Compute mean, median, std dev, and confidence intervals.\"\"\"\n",
    "    arr = np.array(data)\n",
    "    \n",
    "    stats_dict = {\n",
    "        'mean': np.mean(arr),\n",
    "        'median': np.median(arr),\n",
    "        'std': np.std(arr, ddof=1),  # Sample standard deviation\n",
    "        'min': np.min(arr),\n",
    "        'max': np.max(arr),\n",
    "        'q25': np.percentile(arr, 25),\n",
    "        'q75': np.percentile(arr, 75),\n",
    "    }\n",
    "    \n",
    "    # 95% Confidence Interval for the mean\n",
    "    confidence_level = 0.95\n",
    "    degrees_freedom = len(arr) - 1\n",
    "    sample_mean = stats_dict['mean']\n",
    "    sample_std_error = stats.sem(arr)\n",
    "    \n",
    "    confidence_interval = stats.t.interval(\n",
    "        confidence_level,\n",
    "        degrees_freedom,\n",
    "        sample_mean,\n",
    "        sample_std_error\n",
    "    )\n",
    "    \n",
    "    stats_dict['ci_lower'] = confidence_interval[0]\n",
    "    stats_dict['ci_upper'] = confidence_interval[1]\n",
    "    stats_dict['n'] = len(arr)\n",
    "    \n",
    "    return stats_dict\n",
    "\n",
    "def analyze_training_data(filename):\n",
    "    \"\"\"Main analysis function.\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"üìä TRAINING DATA ANALYSIS: ~short Neologism\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    \n",
    "    # Load data\n",
    "    print(f\"Loading data from {filename}...\")\n",
    "    data = load_jsonl(filename)\n",
    "    print(f\"‚úÖ Loaded {len(data)} training examples\\n\")\n",
    "    \n",
    "    # Extract word counts\n",
    "    chosen_word_counts = [count_words(item['chosen']) for item in data]\n",
    "    rejected_word_counts = [count_words(item['rejected']) for item in data]\n",
    "    \n",
    "    # Compute statistics\n",
    "    print(\"=\"*70)\n",
    "    print(\"üìè WORD COUNT STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    \n",
    "    chosen_stats = compute_statistics(chosen_word_counts)\n",
    "    rejected_stats = compute_statistics(rejected_word_counts)\n",
    "    \n",
    "    # Print chosen (short) statistics\n",
    "    print(\"CHOSEN RESPONSES (Short):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Sample Size:        {chosen_stats['n']}\")\n",
    "    print(f\"  Mean:               {chosen_stats['mean']:.2f} words\")\n",
    "    print(f\"  Median:             {chosen_stats['median']:.2f} words\")\n",
    "    print(f\"  Std Dev:            {chosen_stats['std']:.2f} words\")\n",
    "    print(f\"  Min:                {chosen_stats['min']:.0f} words\")\n",
    "    print(f\"  Max:                {chosen_stats['max']:.0f} words\")\n",
    "    print(f\"  25th Percentile:    {chosen_stats['q25']:.2f} words\")\n",
    "    print(f\"  75th Percentile:    {chosen_stats['q75']:.2f} words\")\n",
    "    print(f\"  95% CI:             [{chosen_stats['ci_lower']:.2f}, {chosen_stats['ci_upper']:.2f}]\")\n",
    "    print()\n",
    "    \n",
    "    # Print rejected (long) statistics\n",
    "    print(\"REJECTED RESPONSES (Long):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Sample Size:        {rejected_stats['n']}\")\n",
    "    print(f\"  Mean:               {rejected_stats['mean']:.2f} words\")\n",
    "    print(f\"  Median:             {rejected_stats['median']:.2f} words\")\n",
    "    print(f\"  Std Dev:            {rejected_stats['std']:.2f} words\")\n",
    "    print(f\"  Min:                {rejected_stats['min']:.0f} words\")\n",
    "    print(f\"  Max:                {rejected_stats['max']:.0f} words\")\n",
    "    print(f\"  25th Percentile:    {rejected_stats['q25']:.2f} words\")\n",
    "    print(f\"  75th Percentile:    {rejected_stats['q75']:.2f} words\")\n",
    "    print(f\"  95% CI:             [{rejected_stats['ci_lower']:.2f}, {rejected_stats['ci_upper']:.2f}]\")\n",
    "    print()\n",
    "    \n",
    "    # Comparison\n",
    "    print(\"=\"*70)\n",
    "    print(\"üîÑ COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    print(f\"  Mean Difference:    {rejected_stats['mean'] - chosen_stats['mean']:.2f} words\")\n",
    "    print(f\"  Ratio (Rejected/Chosen): {rejected_stats['mean'] / chosen_stats['mean']:.2f}x\")\n",
    "    print()\n",
    "    \n",
    "    # Statistical test (paired t-test since same prompts)\n",
    "    t_statistic, p_value = stats.ttest_rel(rejected_word_counts, chosen_word_counts)\n",
    "    print(f\"  Paired t-test:\")\n",
    "    print(f\"    t-statistic:      {t_statistic:.4f}\")\n",
    "    print(f\"    p-value:          {p_value:.4e}\")\n",
    "    print(f\"    Significant?      {'Yes' if p_value < 0.05 else 'No'} (Œ±=0.05)\")\n",
    "    print()\n",
    "    \n",
    "    # Under 50 words analysis\n",
    "    print(\"=\"*70)\n",
    "    print(\"üéØ CHOSEN RESPONSES UNDER 50 WORDS\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    \n",
    "    under_50 = sum(1 for count in chosen_word_counts if count < 50)\n",
    "    total = len(chosen_word_counts)\n",
    "    percentage = (under_50 / total) * 100\n",
    "    \n",
    "    print(f\"  Count:              {under_50} / {total}\")\n",
    "    print(f\"  Percentage:         {percentage:.2f}%\")\n",
    "    print()\n",
    "    \n",
    "    # Additional thresholds\n",
    "    thresholds = [25, 50, 75, 100, 150]\n",
    "    print(\"  Distribution by thresholds:\")\n",
    "    print(\"  \" + \"-\" * 40)\n",
    "    for threshold in thresholds:\n",
    "        under_threshold = sum(1 for count in chosen_word_counts if count < threshold)\n",
    "        pct = (under_threshold / total) * 100\n",
    "        print(f\"    < {threshold:3d} words:      {under_threshold:4d} ({pct:5.2f}%)\")\n",
    "    print()\n",
    "    \n",
    "    # Word count distribution\n",
    "    print(\"=\"*70)\n",
    "    print(\"üìä WORD COUNT DISTRIBUTION (Chosen)\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    \n",
    "    bins = [0, 25, 50, 75, 100, 150, 200, 300, float('inf')]\n",
    "    bin_labels = ['0-25', '26-50', '51-75', '76-100', '101-150', '151-200', '201-300', '300+']\n",
    "    \n",
    "    distribution = {label: 0 for label in bin_labels}\n",
    "    for count in chosen_word_counts:\n",
    "        for i, (lower, upper) in enumerate(zip(bins[:-1], bins[1:])):\n",
    "            if lower <= count < upper:\n",
    "                distribution[bin_labels[i]] += 1\n",
    "                break\n",
    "    \n",
    "    for label, count in distribution.items():\n",
    "        pct = (count / total) * 100\n",
    "        bar = '‚ñà' * int(pct / 2)\n",
    "        print(f\"  {label:>10} words: {count:4d} ({pct:5.2f}%) {bar}\")\n",
    "    print()\n",
    "    \n",
    "    # Create visualizations\n",
    "    create_visualizations(chosen_word_counts, rejected_word_counts)\n",
    "    \n",
    "    return {\n",
    "        'chosen_stats': chosen_stats,\n",
    "        'rejected_stats': rejected_stats,\n",
    "        'under_50_count': under_50,\n",
    "        'under_50_percentage': percentage,\n",
    "        'distribution': distribution\n",
    "    }\n",
    "\n",
    "def create_visualizations(chosen_counts, rejected_counts):\n",
    "    \"\"\"Create and save visualization plots.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Training Data Analysis: ~short Neologism', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Histograms\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.hist(chosen_counts, bins=50, alpha=0.7, label='Chosen (short)', color='green', edgecolor='black')\n",
    "    ax1.hist(rejected_counts, bins=50, alpha=0.7, label='Rejected (long)', color='red', edgecolor='black')\n",
    "    ax1.set_xlabel('Word Count')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('Distribution of Word Counts')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Box plots\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.boxplot([chosen_counts, rejected_counts], \n",
    "                labels=['Chosen\\n(short)', 'Rejected\\n(long)'],\n",
    "                patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                medianprops=dict(color='red', linewidth=2))\n",
    "    ax2.set_ylabel('Word Count')\n",
    "    ax2.set_title('Box Plot Comparison')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 3. Scatter plot (index vs word count)\n",
    "    ax3 = axes[1, 0]\n",
    "    indices = range(len(chosen_counts))\n",
    "    ax3.scatter(indices, chosen_counts, alpha=0.5, s=10, label='Chosen', color='green')\n",
    "    ax3.scatter(indices, rejected_counts, alpha=0.5, s=10, label='Rejected', color='red')\n",
    "    ax3.set_xlabel('Training Example Index')\n",
    "    ax3.set_ylabel('Word Count')\n",
    "    ax3.set_title('Word Count by Training Example')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Cumulative distribution\n",
    "    ax4 = axes[1, 1]\n",
    "    chosen_sorted = np.sort(chosen_counts)\n",
    "    rejected_sorted = np.sort(rejected_counts)\n",
    "    chosen_cumulative = np.arange(1, len(chosen_sorted) + 1) / len(chosen_sorted) * 100\n",
    "    rejected_cumulative = np.arange(1, len(rejected_sorted) + 1) / len(rejected_sorted) * 100\n",
    "    \n",
    "    ax4.plot(chosen_sorted, chosen_cumulative, label='Chosen (short)', color='green', linewidth=2)\n",
    "    ax4.plot(rejected_sorted, rejected_cumulative, label='Rejected (long)', color='red', linewidth=2)\n",
    "    ax4.axhline(y=50, color='gray', linestyle='--', alpha=0.5, label='50th percentile')\n",
    "    ax4.set_xlabel('Word Count')\n",
    "    ax4.set_ylabel('Cumulative Percentage (%)')\n",
    "    ax4.set_title('Cumulative Distribution Function')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    output_file = '/mnt/user-data/outputs/short_training_analysis.png'\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    print(f\"üìà Visualization saved to: {output_file}\")\n",
    "    print()\n",
    "    \n",
    "    return output_file\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run analysis\n",
    "    results = analyze_training_data('/mnt/user-data/uploads/short_complete.jsonl')\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"‚úÖ Analysis Complete!\")\n",
    "    print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
