{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f58c1055",
   "metadata": {},
   "source": [
    "# rewrite to be the same as the neo file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e151a304",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RESULTS_PATH = f\"lora_rank{LORA_RANK}_inference_results.jsonl\"\n",
    "\n",
    "results = []\n",
    "with open(RESULTS_PATH, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        results.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(results)} responses\")\n",
    "\n",
    "# Calculate word lengths and token lengths\n",
    "word_lengths = []\n",
    "token_lengths = []\n",
    "\n",
    "for r in results:\n",
    "    response = r['response']\n",
    "    \n",
    "    words = response.split()\n",
    "    word_lengths.append(len(words))\n",
    "    \n",
    "    tokens = tokenizer.encode(response, add_special_tokens=False)\n",
    "    token_lengths.append(len(tokens))\n",
    "\n",
    "word_lengths = np.array(word_lengths)\n",
    "token_lengths = np.array(token_lengths)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"=\" * 60)\n",
    "print(f\"RESPONSE LENGTH STATISTICS (LoRA rank={LORA_RANK})\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nWord Length:\")\n",
    "print(f\"  Mean:   {word_lengths.mean():.2f}\")\n",
    "print(f\"  Stdev:  {word_lengths.std():.2f}\")\n",
    "print(f\"  Min:    {word_lengths.min()}\")\n",
    "print(f\"  Max:    {word_lengths.max()}\")\n",
    "\n",
    "print(f\"\\nToken Length:\")\n",
    "print(f\"  Mean:   {token_lengths.mean():.2f}\")\n",
    "print(f\"  Stdev:  {token_lengths.std():.2f}\")\n",
    "print(f\"  Min:    {token_lengths.min()}\")\n",
    "print(f\"  Max:    {token_lengths.max()}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Histograms\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(word_lengths, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].axvline(word_lengths.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {word_lengths.mean():.1f}')\n",
    "axes[0].set_xlabel('Word Count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title(f'Response Length Distribution - LoRA rank={LORA_RANK} (Words)')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(token_lengths, bins=30, edgecolor='black', alpha=0.7, color='darkorange')\n",
    "axes[1].axvline(token_lengths.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {token_lengths.mean():.1f}')\n",
    "axes[1].set_xlabel('Token Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title(f'Response Length Distribution - LoRA rank={LORA_RANK} (Tokens)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "## Step 10: Side-by-Side Comparison (Optional)\n",
    "\n",
    "If you have neologism results, compare them here.\n",
    "\n",
    "# Load neologism results if available\n",
    "NEOLOGISM_RESULTS_PATH = \"short_inference_results.jsonl\"\n",
    "\n",
    "if os.path.exists(NEOLOGISM_RESULTS_PATH):\n",
    "    neo_results = []\n",
    "    with open(NEOLOGISM_RESULTS_PATH, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            neo_results.append(json.loads(line))\n",
    "    \n",
    "    neo_word_lengths = np.array([len(r['response'].split()) for r in neo_results])\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"COMPARISON: Neologism vs LoRA Fine-tuning\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nNeologism (~short):\")\n",
    "    print(f\"  Mean word count: {neo_word_lengths.mean():.2f}\")\n",
    "    print(f\"  Stdev: {neo_word_lengths.std():.2f}\")\n",
    "    \n",
    "    print(f\"\\nLoRA (rank={LORA_RANK}):\")\n",
    "    print(f\"  Mean word count: {word_lengths.mean():.2f}\")\n",
    "    print(f\"  Stdev: {word_lengths.std():.2f}\")\n",
    "    \n",
    "    print(f\"\\nDifference: {word_lengths.mean() - neo_word_lengths.mean():.2f} words\")\n",
    "    \n",
    "    # Overlaid histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(neo_word_lengths, bins=30, alpha=0.5, label=f'Neologism (mean={neo_word_lengths.mean():.1f})', color='blue')\n",
    "    plt.hist(word_lengths, bins=30, alpha=0.5, label=f'LoRA rank={LORA_RANK} (mean={word_lengths.mean():.1f})', color='orange')\n",
    "    plt.xlabel('Word Count')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Response Length: Neologism vs LoRA Fine-tuning')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Neologism results not found at {NEOLOGISM_RESULTS_PATH}\")\n",
    "    print(\"Run neologism inference first to enable comparison.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
