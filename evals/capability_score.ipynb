{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Capability Score Evaluation\n",
    "\n",
    "Compute capability scores for each model/method using LLM-as-a-Judge.\n",
    "This is a sanity check - no gap closure calculation, just individual scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Capability Score Evaluation using LLM-as-Judge\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# OpenAI installs\n",
    "%pip install openai\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "api-key",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Key\n",
    "OPENAI_API_KEY = \"\"\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toggles-header",
   "metadata": {},
   "source": [
    "## Configuration Toggles\n",
    "\n",
    "Toggle which inference runs to evaluate. Set to `True` to include in evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toggles",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TOGGLES - Select which inference runs to evaluate\n",
    "# ============================================================================\n",
    "\n",
    "# Base model inference\n",
    "EVAL_BASE_MODEL = True\n",
    "\n",
    "# Fine-tuning training data (the target distribution)\n",
    "EVAL_TRAINING_DATA_KIDMODE = True\n",
    "EVAL_TRAINING_DATA_SHORT = True\n",
    "\n",
    "# Neologism inference\n",
    "EVAL_NEOLOGISM_KIDMODE = True\n",
    "EVAL_NEOLOGISM_SHORT = True\n",
    "EVAL_NEOLOGISM_COMBINED = True  # Combined/composition inference\n",
    "\n",
    "# Fine-tuning inference (LoRA)\n",
    "EVAL_FINETUNING_KIDMODE = True\n",
    "EVAL_FINETUNING_SHORT = False  # Set to True when available\n",
    "\n",
    "# Prompting with synthetic data prompts\n",
    "EVAL_PROMPTING_KIDMODE = True\n",
    "EVAL_PROMPTING_SHORT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "file-paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FILE PATHS\n",
    "# ============================================================================\n",
    "\n",
    "# Base model inference\n",
    "BASE_MODEL_FILE = \"../inference/base/base_mistral_inference_results.jsonl\"\n",
    "\n",
    "# Training data files (fine-tuning format)\n",
    "TRAINING_DATA_KIDMODE_FILE = \"../data-prep/kidmode/kidmode_ft.jsonl\"\n",
    "TRAINING_DATA_SHORT_FILE = \"../data-prep/short/short_ft.jsonl\"\n",
    "\n",
    "# Neologism inference results\n",
    "NEOLOGISM_KIDMODE_FILE = \"../inference/neologism/kidmode/mistral_with_kidmode_inference_results.jsonl\"\n",
    "NEOLOGISM_SHORT_FILE = \"../inference/neologism/short/mistral_with_short_inference_results.jsonl\"\n",
    "NEOLOGISM_COMBINED_FILE = \"../inference/composition_inference_results.jsonl\"\n",
    "\n",
    "# Fine-tuning inference results\n",
    "FINETUNING_KIDMODE_FILE = \"../inference/fine-tuning/kidmode_rank8_inference_results.jsonl\"\n",
    "FINETUNING_SHORT_FILE = \"\"  # Update when available\n",
    "\n",
    "# Prompting inference results\n",
    "PROMPTING_KIDMODE_FILE = \"../inference/prompting/prompting_kidmode_inference.jsonl\"\n",
    "PROMPTING_SHORT_FILE = \"../inference/prompting/prompting_short_inference.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-size",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAMPLE SIZE CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Number of examples to score (to save API costs)\n",
    "SAMPLE_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judge-header",
   "metadata": {},
   "source": [
    "## LLM Judge Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judge-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_short_score(instruction, response):\n",
    "    \"\"\"\n",
    "    Score a response for brevity/conciseness using LLM-as-judge (OpenAI).\n",
    "    Returns score from 1-10.\n",
    "    \n",
    "    TODO: Fill in the specific judging prompt for the 'short' concept.\n",
    "    \"\"\"\n",
    "    \n",
    "    judging_prompt = f\"\"\"Rate how correct and relevant this response is on a scale from 1-10.\n",
    "\n",
    "    A score of 10 means:\n",
    "    - The response is factually accurate\n",
    "    - The response directly answers the question\n",
    "\n",
    "    A score of 1 means:\n",
    "    - The response is factually incorrect\n",
    "    - The response discusses things other than the question posed\n",
    "\n",
    "Instruction: {instruction}\n",
    "\n",
    "Response: {response}\n",
    "\n",
    "Provide ONLY a single number from 1-10, with no explanation:\"\"\"\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", \n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": judging_prompt}\n",
    "        ],\n",
    "        max_tokens=10,\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    score_text = completion.choices[0].message.content.strip()\n",
    "    \n",
    "    import re\n",
    "    match = re.search(r'\\d+', score_text)\n",
    "    if match:\n",
    "        score = float(match.group())\n",
    "        return min(max(score, 1), 10)\n",
    "    else:\n",
    "        raise ValueError(f\"Could not parse score from: {score_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scoring-header",
   "metadata": {},
   "source": [
    "## Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scoring-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def score_inference_file(filepath, judge_fn, sample_size=SAMPLE_SIZE, response_key='response', prompt_key='prompt'):\n",
    "    \"\"\"\n",
    "    Load an inference results file and score with LLM judge.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to JSONL file\n",
    "        judge_fn: Judge function to use (judge_kidmode_score, judge_short_score, etc.)\n",
    "        sample_size: Number of examples to score (to save API costs)\n",
    "        response_key: Key for response in JSONL (default 'response')\n",
    "        prompt_key: Key for prompt/instruction in JSONL (default 'prompt')\n",
    "    \"\"\"\n",
    "    print(f\"Loading {filepath}...\")\n",
    "    \n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    \n",
    "    print(f\"  Loaded {len(data)} examples\")\n",
    "    \n",
    "    # Sample for scoring\n",
    "    if len(data) > sample_size:\n",
    "        print(f\"  Sampling {sample_size} examples for scoring\")\n",
    "        sampled_data = random.sample(data, sample_size)\n",
    "    else:\n",
    "        sampled_data = data\n",
    "    \n",
    "    print(f\"  Scoring {len(sampled_data)} examples...\")\n",
    "    \n",
    "    scores = []\n",
    "    for i, ex in enumerate(sampled_data):\n",
    "        try:\n",
    "            prompt = ex.get(prompt_key, ex.get('instruction', ''))\n",
    "            response = ex.get(response_key, ex.get('chosen', ''))\n",
    "            \n",
    "            score = judge_fn(prompt, response)\n",
    "            scores.append(score)\n",
    "            \n",
    "            if (i + 1) % 20 == 0:\n",
    "                print(f\"    Progress: {i+1}/{len(sampled_data)} | Current avg: {np.mean(scores):.2f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: Failed to score example {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    stats = {\n",
    "        'mean': np.mean(scores),\n",
    "        'median': np.median(scores),\n",
    "        'std': np.std(scores),\n",
    "        'count': len(scores),\n",
    "        'scores': scores\n",
    "    }\n",
    "    \n",
    "    print(f\"  Mean score: {stats['mean']:.2f} (std: {stats['std']:.2f})\\n\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "def score_training_data(filepath, judge_fn, sample_size=SAMPLE_SIZE):\n",
    "    \"\"\"\n",
    "    Load training data and score with LLM judge.\n",
    "    Training data uses 'chosen' instead of 'response'.\n",
    "    \"\"\"\n",
    "    return score_inference_file(\n",
    "        filepath, \n",
    "        judge_fn, \n",
    "        sample_size=sample_size,\n",
    "        response_key='chosen',\n",
    "        prompt_key='prompt'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-header",
   "metadata": {},
   "source": [
    "## Run Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-evals",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RUN EVALUATIONS\n",
    "# ============================================================================\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CAPABILITY SCORE EVALUATION\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-base",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Model (evaluate for both concepts)\n",
    "if EVAL_BASE_MODEL:\n",
    "    print(\"### BASE MODEL INFERENCE ###\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    print(\"\\nScoring for KIDMODE concept:\")\n",
    "    results['base_model_kidmode'] = score_inference_file(\n",
    "        BASE_MODEL_FILE, \n",
    "        judge_kidmode_score\n",
    "    )\n",
    "    \n",
    "    print(\"Scoring for SHORT concept:\")\n",
    "    results['base_model_short'] = score_inference_file(\n",
    "        BASE_MODEL_FILE, \n",
    "        judge_short_score\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-training-kidmode",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data - Kidmode\n",
    "if EVAL_TRAINING_DATA_KIDMODE:\n",
    "    print(\"### TRAINING DATA - KIDMODE ###\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    results['training_data_kidmode'] = score_training_data(\n",
    "        TRAINING_DATA_KIDMODE_FILE,\n",
    "        judge_kidmode_score\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-training-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data - Short\n",
    "if EVAL_TRAINING_DATA_SHORT:\n",
    "    print(\"### TRAINING DATA - SHORT ###\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    results['training_data_short'] = score_training_data(\n",
    "        TRAINING_DATA_SHORT_FILE,\n",
    "        judge_short_score\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-neologism-kidmode",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neologism Inference - Kidmode\n",
    "if EVAL_NEOLOGISM_KIDMODE:\n",
    "    print(\"### NEOLOGISM INFERENCE - KIDMODE ###\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    results['neologism_kidmode'] = score_inference_file(\n",
    "        NEOLOGISM_KIDMODE_FILE,\n",
    "        judge_kidmode_score\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-neologism-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neologism Inference - Short\n",
    "if EVAL_NEOLOGISM_SHORT:\n",
    "    print(\"### NEOLOGISM INFERENCE - SHORT ###\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    results['neologism_short'] = score_inference_file(\n",
    "        NEOLOGISM_SHORT_FILE,\n",
    "        judge_short_score\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-neologism-combined",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neologism Inference - Combined\n",
    "if EVAL_NEOLOGISM_COMBINED:\n",
    "    print(\"### NEOLOGISM INFERENCE - COMBINED ###\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Score for kidmode\n",
    "    print(\"\\nScoring for KIDMODE concept:\")\n",
    "    results['neologism_combined_kidmode'] = score_inference_file(\n",
    "        NEOLOGISM_COMBINED_FILE,\n",
    "        judge_kidmode_score\n",
    "    )\n",
    "    \n",
    "    # Score for short\n",
    "    print(\"Scoring for SHORT concept:\")\n",
    "    results['neologism_combined_short'] = score_inference_file(\n",
    "        NEOLOGISM_COMBINED_FILE,\n",
    "        judge_short_score\n",
    "    )\n",
    "    \n",
    "    # Score for combined\n",
    "    print(\"Scoring for COMBINED concept:\")\n",
    "    results['neologism_combined_both'] = score_inference_file(\n",
    "        NEOLOGISM_COMBINED_FILE,\n",
    "        judge_combined_score\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-finetuning-kidmode",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning Inference - Kidmode\n",
    "if EVAL_FINETUNING_KIDMODE:\n",
    "    print(\"### FINE-TUNING INFERENCE - KIDMODE ###\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    results['finetuning_kidmode'] = score_inference_file(\n",
    "        FINETUNING_KIDMODE_FILE,\n",
    "        judge_kidmode_score\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-finetuning-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning Inference - Short\n",
    "if EVAL_FINETUNING_SHORT:\n",
    "    print(\"### FINE-TUNING INFERENCE - SHORT ###\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    results['finetuning_short'] = score_inference_file(\n",
    "        FINETUNING_SHORT_FILE,\n",
    "        judge_short_score\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-prompting-kidmode",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompting - Kidmode\n",
    "if EVAL_PROMPTING_KIDMODE:\n",
    "    print(\"### PROMPTING - KIDMODE ###\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    results['prompting_kidmode'] = score_inference_file(\n",
    "        PROMPTING_KIDMODE_FILE,\n",
    "        judge_kidmode_score\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-prompting-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompting - Short\n",
    "if EVAL_PROMPTING_SHORT:\n",
    "    print(\"### PROMPTING - SHORT ###\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    results['prompting_short'] = score_inference_file(\n",
    "        PROMPTING_SHORT_FILE,\n",
    "        judge_short_score\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SUMMARY TABLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CAPABILITY SCORE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{'Method':<40} {'Mean':>8} {'Median':>8} {'Std':>8} {'N':>6}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for name, stats in results.items():\n",
    "    print(f\"{name:<40} {stats['mean']:>8.2f} {stats['median']:>8.2f} {stats['std']:>8.2f} {stats['count']:>6}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "if len(results) > 0:\n",
    "    # Bar chart of mean scores\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    names = list(results.keys())\n",
    "    means = [results[n]['mean'] for n in names]\n",
    "    stds = [results[n]['std'] for n in names]\n",
    "    \n",
    "    # Color by category\n",
    "    colors = []\n",
    "    for name in names:\n",
    "        if 'base' in name:\n",
    "            colors.append('lightcoral')\n",
    "        elif 'training' in name:\n",
    "            colors.append('lightgreen')\n",
    "        elif 'neologism' in name:\n",
    "            colors.append('lightblue')\n",
    "        elif 'finetuning' in name:\n",
    "            colors.append('plum')\n",
    "        elif 'prompting' in name:\n",
    "            colors.append('wheat')\n",
    "        else:\n",
    "            colors.append('gray')\n",
    "    \n",
    "    bars = ax.bar(range(len(names)), means, color=colors, alpha=0.7, yerr=stds, capsize=3)\n",
    "    \n",
    "    ax.set_xticks(range(len(names)))\n",
    "    ax.set_xticklabels(names, rotation=45, ha='right')\n",
    "    ax.set_ylabel('Capability Score (1-10)')\n",
    "    ax.set_title('Capability Scores by Method')\n",
    "    ax.set_ylim(0, 11)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, mean in zip(bars, means):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., mean + 0.3,\n",
    "                f'{mean:.1f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('capability_scores_visualization.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nVisualization saved to capability_scores_visualization.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "def convert_to_serializable(obj):\n",
    "    \"\"\"Convert numpy types to JSON-serializable Python types.\"\"\"\n",
    "    if isinstance(obj, np.number):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.bool_, bool)):\n",
    "        return bool(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(v) for v in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Save results (without individual scores to keep file small)\n",
    "results_summary = {\n",
    "    name: {k: v for k, v in stats.items() if k != 'scores'}\n",
    "    for name, stats in results.items()\n",
    "}\n",
    "results_summary = convert_to_serializable(results_summary)\n",
    "\n",
    "with open('capability_scores_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"Results saved to capability_scores_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
