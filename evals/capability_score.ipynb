{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Capability Score Evaluation\n",
    "\n",
    "Compute capability scores for each model/method using LLM-as-a-Judge.\n",
    "This evaluates factual correctness and relevance - a sanity check that responses actually answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/owenterry/nlp-neologism-verbalization/.venv/lib/python3.10/site-packages (2.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/owenterry/nlp-neologism-verbalization/.venv/lib/python3.10/site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/owenterry/nlp-neologism-verbalization/.venv/lib/python3.10/site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/owenterry/nlp-neologism-verbalization/.venv/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/owenterry/nlp-neologism-verbalization/.venv/lib/python3.10/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: sniffio in /Users/owenterry/nlp-neologism-verbalization/.venv/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/owenterry/nlp-neologism-verbalization/.venv/lib/python3.10/site-packages (from openai) (4.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/owenterry/nlp-neologism-verbalization/.venv/lib/python3.10/site-packages (from openai) (2.12.5)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/owenterry/nlp-neologism-verbalization/.venv/lib/python3.10/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/owenterry/nlp-neologism-verbalization/.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/owenterry/nlp-neologism-verbalization/.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/owenterry/nlp-neologism-verbalization/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: certifi in /Users/owenterry/nlp-neologism-verbalization/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/owenterry/nlp-neologism-verbalization/.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/owenterry/nlp-neologism-verbalization/.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/owenterry/nlp-neologism-verbalization/.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/owenterry/nlp-neologism-verbalization/.venv/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Capability Score Evaluation using LLM-as-Judge\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# OpenAI installs\n",
    "%pip install openai\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "api-key",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Key\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toggles-header",
   "metadata": {},
   "source": [
    "## Configuration Toggles\n",
    "\n",
    "Toggle which inference runs to evaluate for capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toggles",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# TOGGLES - Select which inference runs to evaluate\n# ============================================================================\n\n# Base model inference\nEVAL_BASE_MODEL = False\n\n# Neologism inference\nEVAL_NEOLOGISM_KIDMODE = False\nEVAL_NEOLOGISM_SHORT = False\nEVAL_NEOLOGISM_COMBINED = False\n\n# Fine-tuning inference (LoRA)\nEVAL_FINETUNING_KIDMODE = False\nEVAL_FINETUNING_SHORT = True  # Set to True when available\n\n# Prompting with synthetic data prompts\nEVAL_PROMPTING_KIDMODE = False\nEVAL_PROMPTING_SHORT = False"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "file-paths",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# FILE PATHS\n# ============================================================================\n\n# Base model inference\nBASE_MODEL_FILE = \"../inference/base/base_mistral_inference_results.jsonl\"\n\n# Neologism inference results\nNEOLOGISM_KIDMODE_FILE = \"../inference/neologism/kidmode/mistral_with_kidmode_inference_results.jsonl\"\nNEOLOGISM_SHORT_FILE = \"../inference/neologism/short/mistral_with_short_inference_results.jsonl\"\nNEOLOGISM_COMBINED_FILE = \"../inference/composition_inference_results.jsonl\"\n\n# Fine-tuning inference results\nFINETUNING_KIDMODE_FILE = \"../inference/fine-tuning/kidmode_rank8_inference_results.jsonl\"\nFINETUNING_SHORT_FILE = \"../inference/fine-tuning/short_rank8_inference_results.jsonl\"\n\n# Prompting inference results\nPROMPTING_KIDMODE_FILE = \"../inference/prompting/prompting_kidmode_inference.jsonl\"\nPROMPTING_SHORT_FILE = \"../inference/prompting/prompting_short_inference.jsonl\"\n\n# Existing results file (to append to instead of overwrite)\nEXISTING_RESULTS_FILE = \"capability_scores_results.json\""
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sample-size",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAMPLE SIZE CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Number of examples to score (to save API costs)\n",
    "SAMPLE_SIZE = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judge-header",
   "metadata": {},
   "source": [
    "## LLM Judge Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "judge-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_capability_score(instruction, response):\n",
    "    \"\"\"\n",
    "    Score a response for factual correctness and relevance using LLM-as-judge.\n",
    "    Returns score from 1-10.\n",
    "    \"\"\"\n",
    "    \n",
    "    judging_prompt = f\"\"\"Rate how correct and relevant this response is on a scale from 1-10.\n",
    "\n",
    "A score of 10 means:\n",
    "- The response is factually accurate\n",
    "- The response directly answers the question\n",
    "\n",
    "A score of 1 means:\n",
    "- The response is factually incorrect\n",
    "- The response discusses things other than the question posed\n",
    "\n",
    "Instruction: {instruction}\n",
    "\n",
    "Response: {response}\n",
    "\n",
    "Provide ONLY a single number from 1-10, with no explanation:\"\"\"\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", \n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": judging_prompt}\n",
    "        ],\n",
    "        max_tokens=10,\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    score_text = completion.choices[0].message.content.strip()\n",
    "    \n",
    "    import re\n",
    "    match = re.search(r'\\d+', score_text)\n",
    "    if match:\n",
    "        score = float(match.group())\n",
    "        return min(max(score, 1), 10)\n",
    "    else:\n",
    "        raise ValueError(f\"Could not parse score from: {score_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scoring-header",
   "metadata": {},
   "source": [
    "## Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "scoring-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def score_inference_file(filepath, judge_fn, sample_size=SAMPLE_SIZE, response_key='response', prompt_key='prompt'):\n",
    "    \"\"\"\n",
    "    Load an inference results file and score with LLM judge.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to JSONL file\n",
    "        judge_fn: Judge function to use (judge_kidmode_score, judge_short_score, etc.)\n",
    "        sample_size: Number of examples to score (to save API costs)\n",
    "        response_key: Key for response in JSONL (default 'response')\n",
    "        prompt_key: Key for prompt/instruction in JSONL (default 'prompt')\n",
    "    \"\"\"\n",
    "    print(f\"Loading {filepath}...\")\n",
    "    \n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    \n",
    "    print(f\"  Loaded {len(data)} examples\")\n",
    "    \n",
    "    # Sample for scoring\n",
    "    if len(data) > sample_size:\n",
    "        print(f\"  Sampling {sample_size} examples for scoring\")\n",
    "        sampled_data = random.sample(data, sample_size)\n",
    "    else:\n",
    "        sampled_data = data\n",
    "    \n",
    "    print(f\"  Scoring {len(sampled_data)} examples...\")\n",
    "    \n",
    "    scores = []\n",
    "    for i, ex in enumerate(sampled_data):\n",
    "        try:\n",
    "            prompt = ex.get(prompt_key, ex.get('instruction', ''))\n",
    "            response = ex.get(response_key, ex.get('chosen', ''))\n",
    "            \n",
    "            score = judge_fn(prompt, response)\n",
    "            scores.append(score)\n",
    "            \n",
    "            if (i + 1) % 20 == 0:\n",
    "                print(f\"    Progress: {i+1}/{len(sampled_data)} | Current avg: {np.mean(scores):.2f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: Failed to score example {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    stats = {\n",
    "        'mean': np.mean(scores),\n",
    "        'median': np.median(scores),\n",
    "        'std': np.std(scores),\n",
    "        'count': len(scores),\n",
    "        'scores': scores\n",
    "    }\n",
    "    \n",
    "    print(f\"  Mean score: {stats['mean']:.2f} (std: {stats['std']:.2f})\\n\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "def score_training_data(filepath, judge_fn, sample_size=SAMPLE_SIZE):\n",
    "    \"\"\"\n",
    "    Load training data and score with LLM judge.\n",
    "    Training data uses 'chosen' instead of 'response'.\n",
    "    \"\"\"\n",
    "    return score_inference_file(\n",
    "        filepath, \n",
    "        judge_fn, \n",
    "        sample_size=sample_size,\n",
    "        response_key='chosen',\n",
    "        prompt_key='prompt'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-header",
   "metadata": {},
   "source": [
    "## Run Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "run-evals",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CAPABILITY SCORE EVALUATION\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# RUN EVALUATIONS\n",
    "# ============================================================================\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CAPABILITY SCORE EVALUATION\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eval-base",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### BASE MODEL INFERENCE ###\n",
      "----------------------------------------\n",
      "Loading ../inference/base/base_mistral_inference_results.jsonl...\n",
      "  Loaded 300 examples\n",
      "  Scoring 300 examples...\n",
      "    Progress: 20/300 | Current avg: 8.10\n",
      "    Progress: 40/300 | Current avg: 8.65\n",
      "    Progress: 60/300 | Current avg: 8.63\n",
      "    Progress: 80/300 | Current avg: 8.84\n",
      "    Progress: 100/300 | Current avg: 8.74\n",
      "    Progress: 120/300 | Current avg: 8.82\n",
      "    Progress: 140/300 | Current avg: 8.94\n",
      "    Progress: 160/300 | Current avg: 8.92\n",
      "    Progress: 180/300 | Current avg: 8.91\n",
      "    Progress: 200/300 | Current avg: 8.84\n",
      "    Progress: 220/300 | Current avg: 8.76\n",
      "    Progress: 240/300 | Current avg: 8.78\n",
      "    Progress: 260/300 | Current avg: 8.74\n",
      "    Progress: 280/300 | Current avg: 8.74\n",
      "    Progress: 300/300 | Current avg: 8.73\n",
      "  Mean score: 8.73 (std: 1.75)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Base Model\n",
    "if EVAL_BASE_MODEL:\n",
    "    print(\"### BASE MODEL INFERENCE ###\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    results['base_model'] = score_inference_file(\n",
    "        BASE_MODEL_FILE, \n",
    "        judge_capability_score\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eval-training-kidmode",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### NEOLOGISM INFERENCE - KIDMODE ###\n",
      "----------------------------------------\n",
      "Loading ../inference/neologism/kidmode/mistral_with_kidmode_inference_results.jsonl...\n",
      "  Loaded 300 examples\n",
      "  Scoring 300 examples...\n",
      "    Progress: 20/300 | Current avg: 8.00\n",
      "    Progress: 40/300 | Current avg: 8.12\n",
      "    Progress: 60/300 | Current avg: 8.10\n",
      "    Progress: 80/300 | Current avg: 8.31\n",
      "    Progress: 100/300 | Current avg: 8.25\n",
      "    Progress: 120/300 | Current avg: 8.36\n",
      "    Progress: 140/300 | Current avg: 8.48\n",
      "    Progress: 160/300 | Current avg: 8.49\n",
      "    Progress: 180/300 | Current avg: 8.47\n",
      "    Progress: 200/300 | Current avg: 8.43\n",
      "    Progress: 220/300 | Current avg: 8.40\n",
      "    Progress: 240/300 | Current avg: 8.40\n",
      "    Progress: 260/300 | Current avg: 8.39\n",
      "    Progress: 280/300 | Current avg: 8.37\n",
      "    Progress: 300/300 | Current avg: 8.37\n",
      "  Mean score: 8.37 (std: 1.53)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Neologism Inference - Kidmode\n",
    "if EVAL_NEOLOGISM_KIDMODE:\n",
    "    print(\"### NEOLOGISM INFERENCE - KIDMODE ###\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    results['neologism_kidmode'] = score_inference_file(\n",
    "        NEOLOGISM_KIDMODE_FILE,\n",
    "        judge_capability_score\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eval-training-short",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### NEOLOGISM INFERENCE - SHORT ###\n",
      "----------------------------------------\n",
      "Loading ../inference/neologism/short/mistral_with_short_inference_results.jsonl...\n",
      "  Loaded 300 examples\n",
      "  Scoring 300 examples...\n",
      "    Progress: 20/300 | Current avg: 8.25\n",
      "    Progress: 40/300 | Current avg: 8.47\n",
      "    Progress: 60/300 | Current avg: 8.52\n",
      "    Progress: 80/300 | Current avg: 8.66\n",
      "    Progress: 100/300 | Current avg: 8.62\n",
      "    Progress: 120/300 | Current avg: 8.72\n",
      "    Progress: 140/300 | Current avg: 8.80\n",
      "    Progress: 160/300 | Current avg: 8.71\n",
      "    Progress: 180/300 | Current avg: 8.65\n",
      "    Progress: 200/300 | Current avg: 8.54\n",
      "    Progress: 220/300 | Current avg: 8.44\n",
      "    Progress: 240/300 | Current avg: 8.46\n",
      "    Progress: 260/300 | Current avg: 8.44\n",
      "    Progress: 280/300 | Current avg: 8.44\n",
      "    Progress: 300/300 | Current avg: 8.47\n",
      "  Mean score: 8.47 (std: 1.70)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Neologism Inference - Short\n",
    "if EVAL_NEOLOGISM_SHORT:\n",
    "    print(\"### NEOLOGISM INFERENCE - SHORT ###\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    results['neologism_short'] = score_inference_file(\n",
    "        NEOLOGISM_SHORT_FILE,\n",
    "        judge_capability_score\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eval-neologism-combined",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### NEOLOGISM INFERENCE - COMBINED ###\n",
      "----------------------------------------\n",
      "Loading ../inference/composition_inference_results.jsonl...\n",
      "  Loaded 300 examples\n",
      "  Scoring 300 examples...\n",
      "    Progress: 20/300 | Current avg: 7.65\n",
      "    Progress: 40/300 | Current avg: 7.33\n",
      "    Progress: 60/300 | Current avg: 7.43\n",
      "    Progress: 80/300 | Current avg: 7.38\n",
      "    Progress: 100/300 | Current avg: 7.25\n",
      "    Progress: 120/300 | Current avg: 7.32\n",
      "    Progress: 140/300 | Current avg: 7.33\n",
      "    Progress: 160/300 | Current avg: 7.29\n",
      "    Progress: 180/300 | Current avg: 7.31\n",
      "    Progress: 200/300 | Current avg: 7.26\n",
      "    Progress: 220/300 | Current avg: 7.25\n",
      "    Progress: 240/300 | Current avg: 7.23\n",
      "    Progress: 260/300 | Current avg: 7.24\n",
      "    Progress: 280/300 | Current avg: 7.28\n",
      "    Progress: 300/300 | Current avg: 7.29\n",
      "  Mean score: 7.29 (std: 1.89)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Neologism Inference - Combined\n",
    "if EVAL_NEOLOGISM_COMBINED:\n",
    "    print(\"### NEOLOGISM INFERENCE - COMBINED ###\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    results['neologism_combined'] = score_inference_file(\n",
    "        NEOLOGISM_COMBINED_FILE,\n",
    "        judge_capability_score\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eval-finetuning-kidmode",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### FINE-TUNING INFERENCE - KIDMODE ###\n",
      "----------------------------------------\n",
      "Loading ../inference/fine-tuning/kidmode_rank8_inference_results.jsonl...\n",
      "  Loaded 300 examples\n",
      "  Scoring 300 examples...\n",
      "    Progress: 20/300 | Current avg: 7.40\n",
      "    Progress: 40/300 | Current avg: 8.03\n",
      "    Progress: 60/300 | Current avg: 7.95\n",
      "    Progress: 80/300 | Current avg: 8.18\n",
      "    Progress: 100/300 | Current avg: 8.22\n",
      "    Progress: 120/300 | Current avg: 8.32\n",
      "    Progress: 140/300 | Current avg: 8.43\n",
      "    Progress: 160/300 | Current avg: 8.24\n",
      "    Progress: 180/300 | Current avg: 8.13\n",
      "    Progress: 200/300 | Current avg: 8.00\n",
      "    Progress: 220/300 | Current avg: 7.99\n",
      "    Progress: 240/300 | Current avg: 7.94\n",
      "    Progress: 260/300 | Current avg: 7.83\n",
      "    Progress: 280/300 | Current avg: 7.86\n",
      "    Progress: 300/300 | Current avg: 7.89\n",
      "  Mean score: 7.89 (std: 2.19)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning Inference - Kidmode\n",
    "if EVAL_FINETUNING_KIDMODE:\n",
    "    print(\"### FINE-TUNING INFERENCE - KIDMODE ###\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    results['finetuning_kidmode'] = score_inference_file(\n",
    "        FINETUNING_KIDMODE_FILE,\n",
    "        judge_capability_score\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eval-finetuning-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning Inference - Short\n",
    "if EVAL_FINETUNING_SHORT:\n",
    "    print(\"### FINE-TUNING INFERENCE - SHORT ###\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    results['finetuning_short'] = score_inference_file(\n",
    "        FINETUNING_SHORT_FILE,\n",
    "        judge_capability_score\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eval-prompting-kidmode",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### PROMPTING - KIDMODE ###\n",
      "----------------------------------------\n",
      "Loading ../inference/prompting/prompting_kidmode_inference.jsonl...\n",
      "  Loaded 300 examples\n",
      "  Scoring 300 examples...\n",
      "    Progress: 20/300 | Current avg: 7.80\n",
      "    Progress: 40/300 | Current avg: 7.95\n",
      "    Progress: 60/300 | Current avg: 7.97\n",
      "    Progress: 80/300 | Current avg: 8.19\n",
      "    Progress: 100/300 | Current avg: 8.28\n",
      "    Progress: 120/300 | Current avg: 8.36\n",
      "    Progress: 140/300 | Current avg: 8.44\n",
      "    Progress: 160/300 | Current avg: 8.31\n",
      "    Progress: 180/300 | Current avg: 8.26\n",
      "    Progress: 200/300 | Current avg: 8.21\n",
      "    Progress: 220/300 | Current avg: 8.17\n",
      "    Progress: 240/300 | Current avg: 8.17\n",
      "    Progress: 260/300 | Current avg: 8.15\n",
      "    Progress: 280/300 | Current avg: 8.16\n",
      "    Progress: 300/300 | Current avg: 8.18\n",
      "  Mean score: 8.18 (std: 1.68)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompting - Kidmode\n",
    "if EVAL_PROMPTING_KIDMODE:\n",
    "    print(\"### PROMPTING - KIDMODE ###\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    results['prompting_kidmode'] = score_inference_file(\n",
    "        PROMPTING_KIDMODE_FILE,\n",
    "        judge_capability_score\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eval-prompting-short",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### PROMPTING - SHORT ###\n",
      "----------------------------------------\n",
      "Loading ../inference/prompting/prompting_short_inference.jsonl...\n",
      "  Loaded 300 examples\n",
      "  Scoring 300 examples...\n",
      "    Progress: 20/300 | Current avg: 7.80\n",
      "    Progress: 40/300 | Current avg: 8.03\n",
      "    Progress: 60/300 | Current avg: 8.27\n",
      "    Progress: 80/300 | Current avg: 8.50\n",
      "    Progress: 100/300 | Current avg: 8.49\n",
      "    Progress: 120/300 | Current avg: 8.54\n",
      "    Progress: 140/300 | Current avg: 8.66\n",
      "    Progress: 160/300 | Current avg: 8.59\n",
      "    Progress: 180/300 | Current avg: 8.47\n",
      "    Progress: 200/300 | Current avg: 8.33\n",
      "    Progress: 220/300 | Current avg: 8.25\n",
      "    Progress: 240/300 | Current avg: 8.26\n",
      "    Progress: 260/300 | Current avg: 8.25\n",
      "    Progress: 280/300 | Current avg: 8.28\n",
      "    Progress: 300/300 | Current avg: 8.31\n",
      "  Mean score: 8.31 (std: 1.87)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompting - Short\n",
    "if EVAL_PROMPTING_SHORT:\n",
    "    print(\"### PROMPTING - SHORT ###\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    results['prompting_short'] = score_inference_file(\n",
    "        PROMPTING_SHORT_FILE,\n",
    "        judge_capability_score\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# LOAD EXISTING RESULTS AND MERGE WITH NEW RESULTS\n# ============================================================================\n\n# Load existing results if available\nexisting_results = {}\nif os.path.exists(EXISTING_RESULTS_FILE):\n    print(f\"Loading existing results from {EXISTING_RESULTS_FILE}...\")\n    with open(EXISTING_RESULTS_FILE, 'r') as f:\n        existing_results = json.load(f)\n    print(f\"  Found {len(existing_results)} existing entries\")\n\n# Merge: existing results + new results (new results overwrite if key exists)\nall_results = {**existing_results, **results}\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"CAPABILITY SCORE SUMMARY\")\nprint(\"=\"*70)\n\nprint(f\"\\n{'Method':<40} {'Mean':>8} {'Median':>8} {'Std':>8} {'N':>6}\")\nprint(\"-\"*70)\n\nfor name, stats in all_results.items():\n    print(f\"{name:<40} {stats['mean']:>8.2f} {stats['median']:>8.2f} {stats['std']:>8.2f} {stats['count']:>6}\")\n\nprint(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# VISUALIZATION (using all_results which includes existing + new)\n# ============================================================================\n\nif len(all_results) > 0:\n    # Bar chart of mean scores\n    fig, ax = plt.subplots(figsize=(14, 6))\n    \n    names = list(all_results.keys())\n    means = [all_results[n]['mean'] for n in names]\n    stds = [all_results[n]['std'] for n in names]\n    \n    # Color by category\n    colors = []\n    for name in names:\n        if 'base' in name:\n            colors.append('lightcoral')\n        elif 'training' in name:\n            colors.append('lightgreen')\n        elif 'neologism' in name:\n            colors.append('lightblue')\n        elif 'finetuning' in name:\n            colors.append('plum')\n        elif 'prompting' in name:\n            colors.append('wheat')\n        else:\n            colors.append('gray')\n    \n    bars = ax.bar(range(len(names)), means, color=colors, alpha=0.7, yerr=stds, capsize=3)\n    \n    ax.set_xticks(range(len(names)))\n    ax.set_xticklabels(names, rotation=45, ha='right')\n    ax.set_ylabel('Capability Score (1-10)')\n    ax.set_title('Capability Scores by Method')\n    ax.set_ylim(0, 11)\n    ax.grid(True, alpha=0.3, axis='y')\n    \n    # Add value labels\n    for bar, mean in zip(bars, means):\n        ax.text(bar.get_x() + bar.get_width()/2., mean + 0.3,\n                f'{mean:.1f}', ha='center', va='bottom', fontsize=9)\n    \n    plt.tight_layout()\n    plt.savefig('capability_scores_visualization.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    print(\"\\nVisualization saved to capability_scores_visualization.png\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# SAVE RESULTS (merged existing + new)\n# ============================================================================\n\ndef convert_to_serializable(obj):\n    \"\"\"Convert numpy types to JSON-serializable Python types.\"\"\"\n    if isinstance(obj, np.number):\n        return float(obj)\n    elif isinstance(obj, (np.bool_, bool)):\n        return bool(obj)\n    elif isinstance(obj, dict):\n        return {k: convert_to_serializable(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        return [convert_to_serializable(v) for v in obj]\n    else:\n        return obj\n\n# Prepare results for saving (without individual scores to keep file small)\n# Start with existing results, then add/update with new results\nresults_summary = {}\n\n# Add existing results\nfor name, stats in existing_results.items():\n    results_summary[name] = {k: v for k, v in stats.items() if k != 'scores'}\n\n# Add/update with new results from this run\nfor name, stats in results.items():\n    results_summary[name] = {k: v for k, v in stats.items() if k != 'scores'}\n\nresults_summary = convert_to_serializable(results_summary)\n\nwith open(EXISTING_RESULTS_FILE, 'w') as f:\n    json.dump(results_summary, f, indent=2)\n\nprint(f\"Results saved to {EXISTING_RESULTS_FILE}\")\nprint(f\"Total entries: {len(results_summary)}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}